{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fetching\n",
    "ðŸ“˜ **Interactive Version**: For a hands-on experience with this chapter's content, access the interactive notebook in [Google Colab](https://drive.google.com/file/d/1VP5pPJZg-buCyLd-Z-tojJop2CJHiGkM/view?usp=sharing).\n",
    "In this section, we will delve into the process of acquiring the dataset that has been integral to our analyses â€“ the OLCI data from the Sentinel-3 satellite, part of the Copernicus Dataspace. This segment will guide you through the nuances of accessing this rich dataset, understanding its structure, and efficiently retrieving the data you need for your work. \n",
    "\n",
    "## Copernicus Data Space\n",
    "\n",
    "**Overview**  \n",
    "Copernicus Data Space is a cornerstone of the European Union's Earth observation program, providing a wealth of data from the Sentinel satellites. Aimed at monitoring the Earth's environment, it supports applications in areas like climate change, disaster response, and urban planning.\n",
    "\n",
    "**Key Features**\n",
    "- **Diverse Datasets**: Offers imagery, atmospheric measurements, and climate indicators.\n",
    "- **Accessibility**: Data is freely accessible, fostering open science and research.\n",
    "\n",
    "**Resources**  \n",
    "For more information and data access, visit the [Copernicus Dataspace](https://dataspace.copernicus.eu).\n",
    "\n",
    "---\n",
    "\n",
    "## Google Earth Engine\n",
    "\n",
    "**Overview**  \n",
    "Google Earth Engine is a powerful platform for analyzing geospatial data at scale. It combines a multi-petabyte catalog of satellite imagery with cloud computing resources, enabling planetary-scale environmental data analysis.\n",
    "\n",
    "**Capabilities**\n",
    "- **Rich Data Catalog**: Access to extensive satellite imagery and geospatial datasets.\n",
    "- **Powerful Computing**: Leverages Google's cloud infrastructure for large-scale data processing.\n",
    "\n",
    "**Resources**  \n",
    "Explore tutorials and community scripts at the [Google Earth Engine's official site](https://earthengine.google.com/).\n",
    "\n",
    "For this notebook, we will use both Google Earth Engine and Copernicus Dataspace to help us to get the raw OLCI dataset.\n",
    "\n",
    "## Set up Accounts\n",
    "\n",
    "Before delving into the specifics of data retrieval, it's crucial to ensure you have access to the necessary platforms. We'll be working with two primary data sources: Google Earth Engine and Copernicus Dataspace.\n",
    "\n",
    "1. **Google Earth Engine:** Most of you might already have a Google account. If so, you're a step ahead! You can use the same account to access Google Earth Engine. \n",
    "\n",
    "2. **Copernicus Dataspace:** Accessing data from the Copernicus Dataspace requires a separate registration. If you haven't done so, please take a moment to create an account. Simply visit the [Copernicus Dataspace registration page](https://dataspace.copernicus.eu) and follow the instructions to sign up. \n",
    "\n",
    "## Data Fetching Logic\n",
    "\n",
    "The process of data fetching is meticulously structured to ensure we efficiently retrieve accurate and comprehensive data. The logic underlying this process involves several key steps:\n",
    "\n",
    "1. **Area and Time Specification:** Initially, we define the geographical scope and the specific time frame of interest. This precise specification allows us to target our data retrieval effectively.\n",
    "\n",
    "2. **Retrieving File Names from Google Earth Engine:** Once the area and time parameters are set, we proceed to fetch a list of relevant file names from Google Earth Engine. This platform provides a robust interface for identifying datasets that match our specified criteria.\n",
    "\n",
    "3. **Converting to Copernicus Filename Format:** After obtaining the list of file names from Google Earth Engine, the next step involves transforming these names into the format recognised by the Copernicus Dataspace. This conversion is crucial for ensuring compatibility and facilitating the subsequent data retrieval process.\n",
    "\n",
    "4. **Fetching Raw Data from Copernicus Dataspace:** With the correctly formatted file names in hand, we then access the Copernicus Dataspace to retrieve the raw data. It's important to note that Google Earth Engine does not provide raw data in the same format as the Copernicus Dataspace. Hence, this step is essential for obtaining the data in its native, unaltered format.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Set Up\n",
    "\n",
    "Before we dive into the data fetching process, it's essential to lay the groundwork by setting up the necessary packages and ensuring proper authentication. Follow these preparatory steps to create a smooth and efficient workflow:\n",
    "\n",
    "1. **Install Required Packages:** Make sure all the necessary packages are installed in your working environment. This includes libraries specific to data handling, geospatial analysis, and any other tools relevant to your project.\n",
    "\n",
    "2. **Authenticate with Google Earth Engine:** Access to Google Earth Engine requires proper authentication. Ensure that you're logged into your Google account and have followed the authentication procedures to obtain access to the datasets and tools offered by Google Earth Engine.\n",
    "\n",
    "\n",
    "By completing these initial setup steps, you're ensuring that your environment is ready and equipped with the tools needed for data fetching and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from shapely.geometry import Polygon, Point\n",
    "import ee \n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import subprocess\n",
    "\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read in Functions Needed\n",
    "\n",
    "To streamline our data fetching and processing, we'll first load the essential functions. These functions are designed to handle various tasks such as data retrieval, format conversion, and preliminary data processing. Ensure that you've imported all the required functions before proceeding to the next steps of the workflow. All functions have docstrings so please read them to get some ideas of what they do.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_gee_filename(gee_filename):\n",
    "    \"\"\"\n",
    "    Parses the Google Earth Engine filename to extract satellite name, sensing date, and start time.\n",
    "\n",
    "    Parameters:\n",
    "    gee_filename (str): Filename obtained from Google Earth Engine.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Contains satellite name, sensing date, and start time.\n",
    "    \"\"\"\n",
    "    parts = gee_filename.split('_')\n",
    "    satellite = parts[0] + '_OL_1_EFR'\n",
    "    start_datetime = parts[1]\n",
    "    end_datetime = parts[2]\n",
    "\n",
    "    # Extract date from the start_datetime (assuming the format is like '20180601T014926')\n",
    "    sensing_date = start_datetime[:8]  \n",
    "    start_time = start_datetime[9:]   \n",
    "\n",
    "    return satellite, sensing_date, start_time\n",
    "\n",
    "def get_access_token(username, password):\n",
    "    \"\"\"\n",
    "    Retrieves access token from Copernicus Dataspace using the provided credentials.\n",
    "\n",
    "    Parameters:\n",
    "    username (str): Username for Copernicus Dataspace.\n",
    "    password (str): Password for Copernicus Dataspace.\n",
    "\n",
    "    Returns:\n",
    "    str: Access token for authenticated sessions.\n",
    "    \"\"\"\n",
    "    url = 'https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token'\n",
    "    data = {\n",
    "        'grant_type': 'password',\n",
    "        'username': username,\n",
    "        'password': password,\n",
    "        'client_id': 'cdse-public'\n",
    "    }\n",
    "    response = requests.post(url, data=data)\n",
    "    response.raise_for_status()\n",
    "    return response.json()['access_token']\n",
    "\n",
    "def query_sentinel3_olci_data(satellite, sensing_date, start_time, token):\n",
    "    \"\"\"\n",
    "    Queries Sentinel-3 OLCI data from Copernicus Data Space based on satellite name, sensing date, and start time.\n",
    "\n",
    "    Parameters:\n",
    "    satellite (str): Name of the satellite.\n",
    "    sensing_date (str): Date of the data sensing.\n",
    "    start_time (str): Start time of the data sensing.\n",
    "    token (str): Access token for authentication.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame containing the query results with details about the Sentinel-3 OLCI data.\n",
    "    \"\"\"\n",
    "    # Convert sensing_date to datetime object and format it for the query\n",
    "    sensing_datetime = datetime.strptime(f'{sensing_date}T{start_time}', '%Y%m%dT%H%M%S')\n",
    "    sensing_datetime = sensing_datetime - timedelta(seconds=1)\n",
    "    \n",
    "    # Construct the request URL using the filter structure provided\n",
    "    url = (\n",
    "        f\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?\"\n",
    "        f\"$filter=contains(Name,'{satellite}') and \"\n",
    "        f\"ContentDate/Start ge {sensing_datetime.strftime('%Y-%m-%dT%H:%M:%S.000Z')} and \"\n",
    "        f\"ContentDate/Start le {(sensing_datetime + timedelta(days=1)).strftime('%Y-%m-%dT%H:%M:%S.000Z')}&\"\n",
    "        f\"$orderby=ContentDate/Start&$top=1000\"\n",
    "    )\n",
    "    headers = {'Authorization': f'Bearer {token}'}\n",
    "\n",
    "    # Print the URL for debugging\n",
    "    print(url)\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.get(url, headers=headers)\n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        # Print error details and return an empty DataFrame if the request failed\n",
    "        print(f\"Error: Unable to fetch data. Status Code: {response.status_code}. Response: {response.text}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Convert the JSON response to a DataFrame\n",
    "    search_results_df = pd.DataFrame.from_dict(response.json()['value'])\n",
    "    \n",
    "    # Convert the 'ContentDate/Start' to datetime objects and sort the results\n",
    "    search_results_df['SensingStart'] = pd.to_datetime(search_results_df['ContentDate'].apply(lambda x: x['Start']))\n",
    "    search_results_df.sort_values(by='SensingStart', inplace=True)\n",
    "\n",
    "    return search_results_df\n",
    "\n",
    "\n",
    "\n",
    "def fetch_S3_images_by_area_and_date(date_range, spatial_extent, area_of_interest):\n",
    "    \"\"\"\n",
    "    Fetches Sentinel-3 OLCI images based on a specified date range and area of interest.\n",
    "    \n",
    "    :param date_range: List containing the start and end dates (e.g., ['2018-06-01', '2018-06-02'])\n",
    "    :param spatial_extent: List containing the spatial extent [min_lon, min_lat, max_lon, max_lat]\n",
    "    :param area_of_interest: ee.Geometry object defining the specific area for which to fetch images\n",
    "    \n",
    "    :return: List of dictionaries, each containing details about a fetched image, including its ID, date, and download URL.\n",
    "    \"\"\"\n",
    "    # Initialize the Earth Engine module\n",
    "    ee.Initialize()\n",
    "\n",
    "    # Define variables for Sentinel-3 OLCI query\n",
    "    S3_product = 'COPERNICUS/S3/OLCI'\n",
    "\n",
    "    # Query for Sentinel-3 data within the specified date range and area of interest\n",
    "    S3_collection = ee.ImageCollection(S3_product) \\\n",
    "        .filterDate(date_range[0], date_range[1]) \\\n",
    "        .filterBounds(area_of_interest)\n",
    "\n",
    "    # Convert S3_collection to a list of image IDs\n",
    "    S3_image_ids = S3_collection.aggregate_array('system:index').getInfo()\n",
    "    S3_images_info = S3_collection.getInfo()['features']\n",
    "    \n",
    "    # Initialize an empty list to store details\n",
    "    S3_image_details = []\n",
    "    \n",
    "    # Iterate through each image in the collection\n",
    "    for img_info in S3_images_info:\n",
    "        # Fetch image ID\n",
    "        image_id = img_info['id']\n",
    "        \n",
    "        # Fetch image date and other properties as needed\n",
    "        image_date = img_info['properties']['system:time_start']  # Example property\n",
    "        \n",
    "        # Append the details to the list\n",
    "        S3_image_details.append({\n",
    "            'id': image_id,\n",
    "            'date': image_date\n",
    "        })\n",
    "\n",
    "    return S3_image_details\n",
    "\n",
    "def download_single_product(product_id, file_name, access_token, download_dir=\"downloaded_products\"):\n",
    "    \"\"\"\n",
    "    Download a single product from the Copernicus Data Space.\n",
    "\n",
    "    :param product_id: The unique identifier for the product.\n",
    "    :param file_name: The name of the file to be downloaded.\n",
    "    :param access_token: The access token for authorization.\n",
    "    :param download_dir: The directory where the product will be saved.\n",
    "    \"\"\"\n",
    "    # Ensure the download directory exists\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    \n",
    "    # Construct the download URL\n",
    "    url = f\"https://zipper.dataspace.copernicus.eu/odata/v1/Products({product_id})/$value\"\n",
    "\n",
    "    # Set up the session and headers\n",
    "    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "    session = requests.Session()\n",
    "    session.headers.update(headers)\n",
    "\n",
    "    # Perform the request\n",
    "    response = session.get(url, headers=headers, stream=True)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Define the path for the output file\n",
    "        output_file_path = os.path.join(download_dir, file_name + \".zip\")\n",
    "\n",
    "        # Stream the content to a file\n",
    "        with open(output_file_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "        print(f\"Downloaded: {output_file_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download product {product_id}. Status Code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Extract Filenames from Google Earth Engine\n",
    "\n",
    "Once you have set up your environment and are authenticated with Google Earth Engine, the next step is to extract the filenames that meet your specific criteria. This involves querying the Google Earth Engine datasets based on your area of interest, time frame, and any other relevant parameters. The code snippet below demonstrates how to perform this task effectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arctic_region_mask_line=np.loadtxt('region_mask_line.txt')\n",
    "# Create a polygon object representing the Arctic Ocean boundary\n",
    "boundary_polygon = Polygon(arctic_region_mask_line)\n",
    "region_mask_line_tt = [(lon, lat) for lon, lat in arctic_region_mask_line]\n",
    "area_of_interest = ee.Geometry.LineString(coords=region_mask_line_tt, proj='EPSG:4326', geodesic=True)\n",
    "date_range = ['2018-06-01', '2018-06-02']\n",
    "spatial_extent = [-180, 60, 180, 90]  # Example coordinates\n",
    "\n",
    "export_tasks = fetch_S3_images_by_area_and_date(date_range, spatial_extent, area_of_interest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Convert Google EE Filenames to Copernicus Format\n",
    "\n",
    "After retrieving the filenames from Google Earth Engine, the next crucial step is to convert these filenames into the format recognized by the Copernicus Dataspace. This conversion is essential for ensuring compatibility and enabling you to query and retrieve the corresponding datasets from Copernicus. Below is an example code snippet that demonstrates this conversion process. Note that you will need your username and password for the Copernicus Dataspace to authenticate and access the required data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example GEE image ID\n",
    "gee_image_id = 'S3B_20180601T182924_20180601T183224'\n",
    "\n",
    "# Parse the GEE filename to get the date and time\n",
    "satellite, sensing_date, start_time = parse_gee_filename(gee_image_id)\n",
    "\n",
    "# Get access token for Copernicus Data Space (ensure your credentials are correct)\n",
    "username = ''\n",
    "password = ''\n",
    "token = get_access_token(username, password)\n",
    "\n",
    "# Query the Copernicus Data Space for the corresponding Sentinel-3 OLCI data\n",
    "s3_olci_data = query_sentinel3_olci_data(satellite, sensing_date, start_time, token)\n",
    "\n",
    "# Print the results\n",
    "print(s3_olci_data['Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After converting the filenames from Google Earth Engine format to Copernicus format and querying the Copernicus dataspace, you might receive multiple filenames in the response. Typically, the relevant filename you are looking for is the first one in the list. Here's how you can access the desired filename and its unique id:\n",
    "\n",
    "```python\n",
    "# Assuming s3_olci_data is a DataFrame or a dictionary containing the query results\n",
    "# Print the first filename and id from the list, which is usually the one we want\n",
    "print(s3_olci_data['Name'][0])\n",
    "print(s3_olci_data['Id'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Download\n",
    "\n",
    "Once you have the correct filename in the Copernicus format, the final step is to download the data. This process involves authenticating with your Copernicus dataspace credentials and sending a request to download the specified file. Below is an example code snippet demonstrating how to perform the download. Ensure that your username and password are accurate and up-to-date to avoid any authentication issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = ''\n",
    "password = ''\n",
    "token = get_access_token(username, password)\n",
    "access_token = token  # Replace with your actual access token\n",
    "download_dir = \"\"  # Replace with your desired download directory\n",
    "product_id = s3_olci_data['Id'][0]\n",
    "file_name = s3_olci_data['Name'][0]\n",
    "# Download the single product\n",
    "download_single_product(product_id, file_name, access_token, download_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until here, you should have the dataset downloaded in the directory you specified."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
