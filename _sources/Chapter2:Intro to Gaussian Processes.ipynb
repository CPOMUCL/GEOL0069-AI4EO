{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Gaussian Processes\n",
    "\n",
    "## Overview and Definition\n",
    "Gaussian Processes (GPs) are closely related to Optimal Interpolation (OI) in the context of statistical modeling and machine learning. Essentially, a Gaussian Process is a collection of random variables, any finite number of which have a joint Gaussian distribution. This makes GPs a natural choice for modeling distributions over functions. They are particularly powerful for regression problems, where the goal is to predict continuous outcomes.\n",
    "\n",
    "The connection between GPs and OI is rooted in their treatment of uncertainty and the use of probabilistic models. Both GPs and OI employ similar concepts of covariance functions (or kernels) to model the relationships in data. In essence, GPs can be seen as a generalization of OI, extending the core ideas of OI to a broader, more flexible framework. This allows for more sophisticated handling of uncertainty and correlations in data, making GPs a versatile tool in various complex data analysis scenarios, especially where the underlying data-generating processes are unknown or hard to specify.\n",
    "\n",
    "## Mathematical Framework\n",
    "\n",
    "### Basic Concepts\n",
    "A Gaussian Process (GP) is essentially an advanced form of a Gaussian (or normal) distribution, but instead of being over simple variables, it's over functions. Imagine a GP as a method to predict or estimate a function based on known data points. \n",
    "\n",
    "In mathematical terms, a GP is defined for a set of function values, where these values follow a Gaussian distribution. Specifically, for any selection of points from a set $X$, the values that a function $f$ takes at these points follow a joint Gaussian distribution.\n",
    "\n",
    "The key to understanding GPs lies in two main concepts:\n",
    "1. **Mean Function**: $m: X \\rightarrow Y$. This function gives the average expected value of the function $f(x)$ at each point $x$ in the set $X$. It's like predicting the average outcome based on the known data.\n",
    "2. **Kernel or Covariance Function**: $k: X \\times X \\rightarrow Y$. This function tells us how much two points in the set $X$ are related or how they influence each other. It's a way of understanding the relationship or similarity between different points in our data.\n",
    "\n",
    "To apply GPs in a practical setting, we typically select several points in our input space $X$, calculate the mean and covariance at these points, and then use this information to make predictions. This process involves working with vectors and matrices derived from the mean and kernel functions to graphically represent the Gaussian Process.\n",
    "\n",
    "**Note**: In mathematical notation, for a set of points $ \\mathbf{X}=x_1, \\ldots, x_N $, the mean vector $ \\mathbf{m} $ and covariance matrix $ \\mathbf{K} $ are constructed from these points using the mean and kernel functions. Each element of $ \\mathbf{m} $ and $ \\mathbf{K} $ corresponds to the mean and covariance values calculated for these points.\n",
    "### Covariance Functions (Kernels)\n",
    "Covariance functions, or kernels, determine how a Gaussian Process (GP) generalizes from observed data. They are fundamental in defining the GP's behavior.\n",
    "\n",
    "- **Concept and Mathematical Representation**:\n",
    "  - Kernels measure the similarity between points in input space. The function $k(x, x')$ computes the covariance between the outputs corresponding to inputs $x$ and $x'$.\n",
    "  - For example, the Radial Basis Function (RBF) kernel is defined as $k(x, x') = \\exp\\left(-\\frac{1}{2l^2} \\| x - x' \\|^2\\right)$, where $l$ is the length-scale parameter.\n",
    "\n",
    "- **Types of Kernels and Their Uses**:\n",
    "  - **RBF Kernel**: Suited for smooth functions. The length-scale $l$ controls how rapidly the correlation decreases with distance.\n",
    "  - **Linear Kernel**: $k(x, x') = x^T x'$, useful for linear relationships.\n",
    "  - **Periodic Kernels**: Capture periodic behavior, expressed as $k(x, x') = \\exp\\left(-\\frac{2\\sin^2(\\pi|x - x'|)}{l^2}\\right)$.\n",
    "  \n",
    "  \n",
    "  In our context, the **RBF Kernel** will be used in most cases. More practical examples are in future chapters. \n",
    "\n",
    "- **Hyperparameter Tuning**:\n",
    "  - Hyperparameters like $l$ in RBF or periodicity in periodic kernels crucially affect GP modeling. Their tuning, often through methods like maximum likelihood, adapts the GP to the specific data structure.\n",
    "\n",
    "- **Choosing the Right Kernel**:\n",
    "  - Involves understanding data characteristics. RBF is a default choice for many, but specific data patterns might necessitate different or combined kernels.\n",
    "\n",
    "\n",
    "\n",
    "### Mean and Variance\n",
    "The mean and variance functions in a Gaussian Process (GP) provide predictions and their uncertainties.\n",
    "\n",
    "- **Mean Function - Mathematical Explanation**:\n",
    "  - The mean function, often denoted as $m(x)$, gives the expected value of the function at each point. A common assumption is $m(x) = 0$, although non-zero means can incorporate prior trends. \n",
    "\n",
    "- **Variance Function - Quantifying Uncertainty**:\n",
    "  - The variance, denoted as $\\sigma^2(x)$, represents the uncertainty in predictions. It's calculated as $\\sigma^2(x) = k(x, x) - K(X, x)^T[K(X, X) + \\sigma^2_nI]^{-1}K(X, x)$, where $K(X, x)$ and $K(X, X)$ are covariance matrices, and $\\sigma^2_n$ is the noise term.\n",
    "\n",
    "- **Practical Interpretation**:\n",
    "  - High variance at a point suggests low confidence in predictions there, guiding decisions on where more data might be needed or caution in using the predictions.\n",
    "\n",
    "- **Mean and Variance in Predictions**:\n",
    "  - Together, they provide a probabilistic forecast. The mean offers the best guess, while the variance indicates reliability. This duo is key in risk-sensitive applications.\n",
    "\n",
    "\n",
    "## Gaussian Process - A Logical Processing Chain\n",
    "\n",
    "Just like other machine learning algorithm, the logical processing chain for a Gaussian Process (GP) involves thoese key steps:\n",
    "\n",
    "1. **Defining the Problem**:\n",
    "   - Start by identifying the problem to be solved using GP, such as regression, classification, or another task where predicting a continuous function is required.\n",
    "\n",
    "2. **Data Preparation**:\n",
    "   - Organise the data into a suitable format. This includes input features and corresponding target values.\n",
    "\n",
    "3. **Choosing a Kernel Function**:\n",
    "   - Select an appropriate kernel (covariance function) for the GP. The choice depends on the nature of the data and the problem.\n",
    "\n",
    "4. **Setting the Hyperparameters**:\n",
    "   - Initialise hyperparameters for the chosen kernel. These can include parameters like length-scale in the RBF kernel or periodicity in a periodic kernel.\n",
    "\n",
    "5. **Model Training**:\n",
    "   - Train the GP model by optimizing the hyperparameters. This usually involves maximizing the likelihood of the observed data under the GP model.\n",
    "\n",
    "6. **Prediction**:\n",
    "   - Use the trained GP model to make predictions. This involves computing the mean and variance of the GPâ€™s posterior distribution.\n",
    "\n",
    "7. **Model Evaluation**:\n",
    "   - Evaluate the model's performance using suitable metrics. For regression, this could be RMSE or MAE; for classification, accuracy or AUC.\n",
    "\n",
    "8. **Refinement**:\n",
    "   - Based on the evaluation, refine the model by adjusting hyperparameters or kernel choice, and retrain if necessary.\n",
    "\n",
    "This chain provides a comprehensive overview of the steps involved in applying Gaussian Processes to a problem, from initial setup to prediction and evaluation.\n",
    "\n",
    "### Practical Examples\n",
    "You've now covered the essential concepts of Gaussian Processes. Next, let's dive into a practical application by exploring a toy example of GP implementation using the GPyTorch library.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
