{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to GPSat\n",
    "In the previous session, we've looked at some basics in Gaussian Processes. As discussed, we will dive into GPSat in this session. Let's now begin with some recap about the GP basics and get familar with the GPsat Model API to get get/set parameters.\n",
    "\n",
    "First of all, please run the following code to install GPSat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# TODO: allow for mounting of gdrive\n",
    "# TODO: allow for checking out a branch\n",
    "\n",
    "if IN_COLAB:\n",
    "    \n",
    "    import os\n",
    "    import re\n",
    "\n",
    "    # change to working directory\n",
    "    work_dir = \"/content\"\n",
    "    \n",
    "    assert os.path.exists(work_dir), f\"workspace directory: {work_dir} does not exist\"\n",
    "    os.chdir(work_dir)\n",
    "    \n",
    "    # clone repository\n",
    "    !git clone https://github.com/CPOMUCL/GPSat.git\n",
    "\n",
    "    repo_dir = os.path.join(work_dir, \"GPSat\")\n",
    "\n",
    "    print(f\"changing directory to: {repo_dir}\")\n",
    "    os.chdir(repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from GPSat.models.sklearn_models import sklearnGPRModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider data generated from a simple cosine function\n",
    "\n",
    "$$\n",
    "\\tag{1}\n",
    "y = \\cos(X) + \\epsilon,\n",
    "$$\n",
    "\n",
    "where $X = (x_1, \\ldots, x_N)$ is a set of randomly generated points within the interval $[-L, L]$, and $\\epsilon$ is a measurement error, which we take to be an i.i.d. zero-mean Gaussian noise with standard deviation $0.05$.\n",
    "\n",
    "Our goal is to use a Gaussian process model to filter out the noise $\\epsilon$ and recover the function $f(x) = \\cos(x)$ from the training data $(X, y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate data\n",
    "N = 30\n",
    "L = 5\n",
    "noise_std = 0.05\n",
    "\n",
    "X_grid = np.linspace(-L, L, 100)\n",
    "X = np.random.uniform(-L, L, (N,))\n",
    "f = lambda x: np.cos(x)\n",
    "epsilon = noise_std * np.random.randn(N)\n",
    "\n",
    "y = f(X) + epsilon\n",
    "f_truth = f(X_grid) # Ground truth\n",
    "\n",
    "# Plot\n",
    "plt.plot(X_grid, f_truth, 'k', zorder=1, label='Ground truth')\n",
    "plt.scatter(X, y, color='C3', alpha=0.6, zorder=2, label='Noisy observations')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian processes\n",
    "\n",
    "Intuitively, a zero-mean Gaussian process (GP) can be understood as a Gaussian distribution on an *arbitrary collection of inputs*.\n",
    "\n",
    "More specifically, it is a random function $f : \\mathbb{R} \\rightarrow \\mathbb{R}$ such that for an arbitrary collection of inputs $X = (x_1, \\ldots, x_N)$, the random variable $f(X)$ is a multivariate Gaussian\n",
    "\n",
    "$$\n",
    "f(X) \\sim \\mathcal{N}(0, K_{XX}),\n",
    "$$\n",
    "\n",
    "\n",
    "for some $N \\times N$ covariance matrix $K_{XX}$. Importantly, this covariance matrix can be computed using a **kernel function**  $k : \\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R}$ as\n",
    "\n",
    "$$\n",
    "[K_{XX}]_{ij} = k(x_i, x_j), \\quad \\forall i,j = 1, \\ldots, N,\n",
    "$$\n",
    "\n",
    "and this completely characterises the zero-mean GP. i.e. the properties of a GP are completely determined by the kernel!\n",
    "\n",
    "Below, we set up a GP with the so-called radial basis function (RBF) kernel, given as\n",
    "\n",
    "$$\n",
    "\\tag{RBF}\n",
    "k_{\\text{RBF}}(x, x') = \\sigma^2 \\exp\\left(-\\frac{|x-x'|^2}{2\\ell^2}\\right),\n",
    "$$\n",
    "\n",
    "using ``sklearnGPRModel``, a GPSat GP regression model based on the ``scikit-learn`` GPR module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr = sklearnGPRModel(coords=X, obs=y, kernel='RBF', verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the expression for the kernel (RBF) above, we see that it is controlled by two parameters $\\sigma^2$ and $\\ell$, which are referred to as the *kernel variance* and the *lengthscale* hyperparameters respectively (in machine learning lingo, we refer to parameters that define the models as *hyperparameters*).\n",
    "\n",
    "Every ``GPSat`` model comes equipped with a getter/setter method for all (hyper)-parameters in the model. A list of all parameters is stored in the ``param_names`` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpr.param_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve their values using the ``get_*()`` method, where ``*`` is to be substituted with the parameter name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = gpr.get_lengthscales()\n",
    "kv = gpr.get_kernel_variance()\n",
    "\n",
    "print(f\"Lengthscale: {ls}, Kernel variance: {kv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to set the kernel variance to 1.5. We can achieve this using the ``set_*()`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr.set_kernel_variance(1.5)\n",
    "kv = gpr.get_kernel_variance()\n",
    "print(f\"New kernel variance: {kv:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The likelihood\n",
    "In ``param_names`` above, we also saw a parameter ``likelihood_variance``. This is not a hyperparameter of the GP kernel, but is instead a parameter of the so-called *likelihood*.\n",
    "\n",
    "In general, the likelihood describes the probability of an observation $y$ given the ground truth field $f(X)$, i.e., the conditional distribution $p(y | f(X))$. In our case, since the observations are assumed to only differ from the ground truth by some measurement error, the likelihood is understood as modelling precisely this measurement error.\n",
    "\n",
    "From (1), we see that the likelihood is given by\n",
    "\n",
    "$$\n",
    "p(y | f(X)) \\sim \\mathcal{N}(f(X), \\alpha^2 I),\n",
    "$$\n",
    "\n",
    "with $\\alpha = 0.05$ and $f(x) = \\cos(x)$. Here, the parameter $\\alpha^2$ is referred to as the *likelihood variance*.\n",
    "\n",
    "We can get the default value for the likelihood variance using the ``get_*`` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Likelihood variance: {gpr.get_likelihood_variance()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and set the correct value by using the ``set_*`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "gpr.set_likelihood_variance(alpha**2)\n",
    "print(f\"New likelihood variance: {gpr.get_likelihood_variance():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could have also initialised the GPR model by specifying the ``likelihood_variance`` and ``kernel_variance`` arguments with their respective values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This initialises a GP model with the desired values\n",
    "gpr = sklearnGPRModel(coords=X, obs=y, kernel='RBF', likelihood_variance=alpha**2, kernel_variance=1.5, verbose=False)\n",
    "\n",
    "ls = gpr.get_lengthscales()\n",
    "kv = gpr.get_kernel_variance()\n",
    "lv = gpr.get_likelihood_variance()\n",
    "\n",
    "print(f\"Lengthscale: {ls},  Kernel variance: {kv:.1f},  Likelihood variance: {lv:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "From just these information, we can now infer what our ground truth function $f$ should be, given the data pair $(X, y)$.\n",
    "\n",
    "Mathematically this is achieved by the simple, yet powerful **Bayes' rule** to update our belief on the function $f$ given our data $(X, y)$. Informally, this reads:\n",
    "\n",
    "$$\n",
    "\\tag{2}\n",
    "\\underbrace{p(f \\,|\\, X, y)}_{\\text{posterior}} \\propto \\underbrace{p(y \\,|\\, f(X))}_{\\text{likelihood}} \\,\\, \\underbrace{p(f)}_{\\text{prior}}.\n",
    "$$\n",
    "\n",
    "In GP regression, one can understand the GP as modelling a prior distribution on the function $f$. Thus, the term $p(f)$ corresponds to our GP model. The posterior distribution $p(f | X, y)$ thus gives our prediction of the field $f$ given the data.\n",
    "\n",
    "In ``GPSat`` models, this is computed using the ``predict()`` method. This takes as inputs a set of $N_*$ prediction points, which must be an array of size $(N_*, D)$, where $D$ is the input dimension (in our case, just 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on a uniform grid\n",
    "pred_dict = gpr.predict(X_grid.reshape(-1, 1))\n",
    "\n",
    "# Extract the mean and variance from the results dictionary\n",
    "f_mean = pred_dict['f*']\n",
    "f_var = pred_dict['f*_var']\n",
    "f_std = np.sqrt(f_var)\n",
    "\n",
    "# Plot results\n",
    "plt.plot(X_grid, f_truth, 'k', zorder=0, label='Ground truth')\n",
    "plt.plot(X_grid, f_mean, color='C0', zorder=1, label='GP Prediction')\n",
    "plt.fill_between(X_grid, f_mean-1.96*f_std, f_mean+1.96*f_std, color='C0', alpha=0.3)\n",
    "plt.scatter(X, y, color='C3', alpha=0.6, zorder=2, label='Noisy observations')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above, we have plotted the prediction in blue, where the shaded region indicates a 90% credible interval, where we believe the ground truth lies.\n",
    "\n",
    "To assess the quality of this prediction, we can look at two metrics:\n",
    "\n",
    "(1) the mean squared error $\\frac{1}{N_*} \\sum_{n=1}^{N^*} (f_{truth}(x_n') - f_{mean}(x_n'))^2$ between the predictive mean and the ground truth, and\n",
    "\n",
    "(2) the mean log-likelihood $\\frac{1}{N_*} \\sum_{n=1}^{N^*} \\log \\mathcal{N}(f_{truth}(x_n') \\,|\\, f_{mean}(x_n'), f_{std}(x_n')^2)$ of the ground truth given the predictive mean and standard deviation.\n",
    "\n",
    "The former only assess the quality of the mean, however the latter also assess the quality of the predictive uncertainty. For the log-likelihood loss, a higher value indicates better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean squared error: {np.mean((f_truth - f_mean)**2):.4f}\")\n",
    "print(f\"Mean log likelihood: {scipy.stats.norm.logpdf(f_truth, f_mean, f_std).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "To further improve our predictions, we can think of finding a value for the hyperparameters $\\Theta = (\\sigma^2, \\ell, \\alpha^2)$ that fit the data \"better\". This is called the *training* process.\n",
    "\n",
    "To define what a \"better\" model means, we can compare them using a certain metric. A perferred such metric in Bayesian modelling is the so-called *marginal likelihood*, defined as:\n",
    "\n",
    "$$\n",
    "\\tag{3}\n",
    "p(y | \\Theta)  = \\int p(y | f(X), \\Theta) \\,p(f(X) | \\Theta) \\,df(X).\n",
    "$$\n",
    "\n",
    "Thus, we can find an optimal set of parameters by maximising (3) with respect to $\\Theta$. Equivalently, we can also maximise their log-transformed counterpart, which is more typically used.\n",
    "\n",
    "In ``GPSat`` models, we can compute the log-transformed version of the metric (3) by simply calling the ``get_objective_function_value()`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"log marginal likelihood = {gpr.get_objective_function_value():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's optimise this loss function, which can be achieved in ``GPSat`` model by calling the ``optimise_parameters()`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimise model\n",
    "opt_success = gpr.optimise_parameters()\n",
    "\n",
    "# Print outputs\n",
    "print(f\"Optimise success: {opt_success}\")\n",
    "print(\"-\"*30)\n",
    "param_dict = gpr.get_parameters(*gpr.param_names)\n",
    "print(\"Values of model hyperparameters after training:\")\n",
    "for k, v in param_dict.items():\n",
    "    print(f\"{k} : {v:.4f}\")\n",
    "print(\"-\"*30)\n",
    "print(f\"log marginal likelihood (after training) = {gpr.get_objective_function_value():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that after training, the values of the lengthscale and kernel variance hyperparameters have changed. In addition, the log marginal likelihood value has increased.\n",
    "\n",
    "**Note:** For scikit-learn models, the likelihood variance is assumed constant and does not get optimised. If you want to optimise the likelihood variance, use e.g. ``GPSat.models.gpflow_models.GPflowGPRModel`` instead.\n",
    "\n",
    "Now let's see the updated predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict again\n",
    "pred_dict = gpr.predict(X_grid[:,None])\n",
    "\n",
    "# Extract mean, variance and standard deviation\n",
    "f_mean = pred_dict['f*']\n",
    "f_var = pred_dict['f*_var']\n",
    "f_std = np.sqrt(f_var)\n",
    "\n",
    "# Plot predictions\n",
    "plt.plot(X_grid, f_truth, 'k', zorder=0, label='Ground truth')\n",
    "plt.plot(X_grid, f_mean, color='C0', zorder=1, label='GP Prediction')\n",
    "plt.fill_between(X_grid, f_mean-1.96*f_std, f_mean+1.96*f_std, color='C0', alpha=0.3)\n",
    "plt.scatter(X, y, color='C3', alpha=0.6, zorder=2, label='Noisy observations')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the uncertainty bounds are now tighter around the ground truth after training, although the mean prediction is a little bit more off than before.\n",
    "\n",
    "This is reflected in the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean squared error: {np.mean((f_truth - f_mean)**2):.4f}\")\n",
    "print(f\"Mean log likelihood: {scipy.stats.norm.logpdf(f_truth, f_mean, f_std).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPUs to accelerate training and inference\n",
    "\n",
    "Now, with some basics knowledge in Gaussian Processes, we will see the advantages of using GPUs to do training and inference on GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from GPSat.models.sklearn_models import sklearnGPRModel\n",
    "from GPSat.models.gpflow_models import GPflowGPRModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the experiment, we use the same model as before but sampling more data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate data\n",
    "N = 2500\n",
    "L = 5\n",
    "noise_std = 0.05\n",
    "\n",
    "X_grid = np.linspace(-L, L, 100)\n",
    "X = np.random.uniform(-L, L, (N,))\n",
    "f = lambda x: np.cos(x)\n",
    "epsilon = noise_std * np.random.randn(N)\n",
    "\n",
    "y = f(X) + epsilon\n",
    "f_truth = f(X_grid) # Ground truth\n",
    "\n",
    "# Plot\n",
    "plt.plot(X_grid, f_truth, 'k', zorder=1, label='Ground truth')\n",
    "plt.scatter(X, y, color='C3', alpha=0.6, zorder=2, s=0.5, label='Noisy observations')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we model this data using ``sklearnGPRModel``. Scikit-learn does not have GPU support so this will do all the training and prediciton on CPU, even if it detects GPUs on the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# Training on CPU\n",
    "# ---------------\n",
    "\n",
    "# Initialise sklearn GPR model\n",
    "sklearn_gpr = sklearnGPRModel(coords=X, obs=y, kernel='RBF', likelihood_variance=noise_std**2, verbose=False)\n",
    "\n",
    "# Train model\n",
    "_ = sklearn_gpr.optimise_parameters()\n",
    "\n",
    "# Predict on test points\n",
    "pred_dict_sklearn = sklearn_gpr.predict(X_grid[:,None])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use ``GPflowGPRModel`` to model the same data, which is based on the python package ``GPflow``, itself a tensorflow based package for modelling with GPs. This automatically does the training and prediction on GPUs, if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# Training on GPU\n",
    "# ---------------\n",
    "\n",
    "# Initialise GPflow GPR model\n",
    "gpflow_gpr = GPflowGPRModel(coords=X, obs=y, kernel='RBF', noise_variance=noise_std**2, verbose=False)\n",
    "\n",
    "# Train model\n",
    "_ = gpflow_gpr.optimise_parameters(fixed_params=['likelihood_variance'])\n",
    "\n",
    "# Predict on test points\n",
    "pred_dict_gpflow = gpflow_gpr.predict(X_grid[:,None])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In ``GPflowGPRModel``, the likelihood variance is initialised with the argument ``noise_variance`` instead of ``likelihood_variance`` as in ``sklearnGPRModel``. Note also that since likelihood variance is a trainable parameter in ``GPflowGPRModel``, we pass an additional argument ``fixed_params = ['likelihood_variance']`` to the ``optimise_parameters()`` method to freeze the assigned likelhood variance value. For more details, see the API references for both models.\n",
    "\n",
    "We see that training time is much shorter on a GPU than on CPU, which is where most of the computation takes place in the enitre GP workflow. Hence, when we have a large dataset, it is advantageous to use GPUs over CPUs.\n",
    "\n",
    "We can also check that the results of the two predictions are near identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract mean and variance for the sklearn prediction\n",
    "f_mean_sklearn = pred_dict_sklearn['f*']\n",
    "f_var_sklearn = pred_dict_sklearn['f*_var']\n",
    "f_std_sklearn = np.sqrt(f_var_sklearn)\n",
    "\n",
    "# Extract mean and variance for the gpflow prediction\n",
    "f_mean_gpflow = pred_dict_gpflow['f*']\n",
    "f_var_gpflow = pred_dict_gpflow['f*_var']\n",
    "f_std_gpflow = np.sqrt(f_var_gpflow)\n",
    "\n",
    "# Plot results\n",
    "plt.plot(X_grid, f_truth, 'k', zorder=0, label='Ground truth')\n",
    "plt.plot(X_grid, f_mean_sklearn, color='C0', zorder=1, label='sklearn Prediction')\n",
    "plt.fill_between(X_grid, f_mean_sklearn-1.96*f_std_sklearn, f_mean_sklearn+1.96*f_std_sklearn, color='C0', alpha=0.6)\n",
    "plt.plot(X_grid, f_mean_gpflow, color='C1', zorder=1, label='gpflow Prediction')\n",
    "plt.fill_between(X_grid, f_mean_gpflow-1.96*f_std_gpflow, f_mean_gpflow+1.96*f_std_gpflow, color='C1', alpha=0.2)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPSat modelling with local GP experts: A 1-D case study (Part 1)\n",
    "\n",
    "The main intended use case of ``GPSat`` is to model complex looking fields from a large set of data points, a situation commonly encountered in the geosciences, namely optimal interpolation (OI). The strategy that we adopted is quite simple: to model local chunks of data using different GPs and then gluing their predictions together.\n",
    "\n",
    "In this tutorial notebook, we will see how this method performs compared to a single \"global\" GP. We note that ``GPSat`` has an automated API to carry out this whole workflow, which we will see in the second part of this tutorial. However for the sake of understanding, we will hard-code this method here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate noisy data as follows:\n",
    "\n",
    "$$\n",
    "\\tag{1}\n",
    "y = \\sin(1/X) + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 0.05^2 I),\n",
    "$$\n",
    "\n",
    "in the region $X \\in [0.1, 0.6]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate data\n",
    "N = 100\n",
    "noise_std = 0.05\n",
    "\n",
    "X_grid = np.linspace(0.1, 0.6, 100)\n",
    "X = np.random.uniform(0.1, 0.6, (N,))\n",
    "f = lambda x: np.sin(1/x)\n",
    "epsilon = noise_std * np.random.randn(N)\n",
    "\n",
    "y = f(X) + epsilon\n",
    "f_truth = f(X_grid) # Ground truth\n",
    "\n",
    "# Plot\n",
    "plt.plot(X_grid, f_truth, 'k', zorder=1, label='Ground truth')\n",
    "plt.scatter(X, y, color='C3', alpha=0.6, zorder=2, label='Noisy observations')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is tricky to model well as the variability of the curve varies depending on where you are (shorter lengthscales near 0 and longer lengthscales near 1).\n",
    "\n",
    "Let's first see how a standard GP fits on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise sklearn GPR model\n",
    "gpr = sklearnGPRModel(coords=X, obs=y, kernel='RBF', likelihood_variance=noise_std**2, verbose=False)\n",
    "\n",
    "# Train model\n",
    "_ = gpr.optimise_parameters()\n",
    "\n",
    "# Predict on test locations\n",
    "pred_dict = gpr.predict(X_grid[:,None])\n",
    "\n",
    "# Extract mean and variance of predictions\n",
    "f_mean = pred_dict['f*']\n",
    "f_var = pred_dict['f*_var']\n",
    "f_std = np.sqrt(f_var)\n",
    "\n",
    "# Plot results\n",
    "plt.plot(X_grid, f_truth, 'k', zorder=0, label='Ground truth')\n",
    "plt.plot(X_grid, f_mean, color='C0', zorder=1, label='sklearn GPR prediction')\n",
    "plt.fill_between(X_grid, f_mean-1.96*f_std, f_mean+1.96*f_std, color='C0', alpha=0.3)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model fits quite well near 0.1, however as it approaches 0.6, we start to see some spurious fluctuations that does not exist in the ground truth field.\n",
    "\n",
    "Checking the learned lengthscale,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Lengthscale: {gpr.get_lengthscales():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is quite short, which explains the rapid fluctuations.\n",
    "\n",
    "Let us also check the mean squared error and the log-likelihood loss from the ground truth field for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean squared error: {np.mean((f_truth - f_mean)**2):.4f}\")\n",
    "print(f\"Mean log likelihood: {scipy.stats.norm.logpdf(f_truth, f_mean, f_std).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local GP experts\n",
    "Next, let us consider a natural idea to solve this issue by fitting different GPs (called *local experts*) on different regions of the domain. For simplicity, we will use two GP experts: one to model the data for smaller values of $X$ and another to model the data for larger values of $X$.\n",
    "\n",
    "We assign the following data to the two GPs (call it GP1 and GP2):\n",
    "\n",
    "- GP1 gets assigned data points within the interval [0.1, 0.4] and makes predictions in the same region.\n",
    "- GP2 gets assigned data points within the interval [0.3, 0.6] and makes predictions in the same region.\n",
    "\n",
    "Note that we deliberately let the two regions overlap, which will be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data points assigned to GP1 and GP2\n",
    "Data1 = np.array([[x, y] for (x, y) in zip(X, y) if x < 0.4])\n",
    "Data2 = np.array([[x, y] for (x, y) in zip(X, y) if x > 0.3])\n",
    "\n",
    "# Prediction points assigned to GP1 and GP2\n",
    "X_test_1 = X_grid[X_grid < 0.4]\n",
    "X_test_2 = X_grid[X_grid > 0.3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention, we will associate each region ([0.1, 0.4] and [0.3, 0.6]) by their mid-points (0.25 and 0.45 respectively), and refer to them as the *local expert locations*.\n",
    "\n",
    "The distance from the local expert location to the boundary of the region where data points are assigned is referred to as the *training radius*. In this case, we can check that our two experts both have a training radius of 0.15.\n",
    "\n",
    "Likewise, the distance from the local expert location to the boundary of the region where prediction points are assigned is referred to as the *inference radius*. In our case, we have set the inference radius to be equal to the training radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set expert locations\n",
    "xpert_loc_1 = 0.25\n",
    "xpert_loc_2 = 0.45\n",
    "\n",
    "# Set training and inference radii\n",
    "training_radius = inference_radius = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train GP1 and GP2 in their respective regions and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and predict with GP1\n",
    "gp1 = sklearnGPRModel(coords=Data1[:,0], obs=Data1[:,1], kernel='RBF', likelihood_variance=noise_std**2, verbose=False)\n",
    "_ = gp1.optimise_parameters()\n",
    "pred_dict_1 = gp1.predict(X_test_1[:,None])\n",
    "f_mean_1 = pred_dict_1['f*']\n",
    "f_var_1 = pred_dict_1['f*_var']\n",
    "f_std_1 = np.sqrt(f_var_1)\n",
    "\n",
    "# Train and predict with GP2\n",
    "gp2 = sklearnGPRModel(coords=Data2[:,0], obs=Data2[:,1], kernel='RBF', likelihood_variance=noise_std**2, verbose=False)\n",
    "_ = gp2.optimise_parameters()\n",
    "pred_dict_2 = gp2.predict(X_test_2[:,None])\n",
    "f_mean_2 = pred_dict_2['f*']\n",
    "f_var_2 = pred_dict_2['f*_var']\n",
    "f_std_2 = np.sqrt(f_var_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we plot the predictions from the two GPs over-layed on top of one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.plot(X_grid, f_truth, 'k', zorder=0, label='Ground truth')\n",
    "plt.plot(X_test_1, f_mean_1, color='C0', zorder=1, label='Local expert 1')\n",
    "plt.fill_between(X_test_1, f_mean_1-1.96*f_std_1, f_mean_1+1.96*f_std_1, color='C0', alpha=0.3)\n",
    "plt.plot(X_test_2, f_mean_2, color='C1', zorder=1, label='Local expert 2')\n",
    "plt.fill_between(X_test_2, f_mean_2-1.96*f_std_2, f_mean_2+1.96*f_std_2, color='C1', alpha=0.3)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we find that GP1 fits the data with a shorter lengthscale and GP2 fits the data with a longer lengthscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Lengthscale of GP1: {gp1.get_lengthscales():.4f}\")\n",
    "print(f\"Lengthscale of GP2: {gp2.get_lengthscales():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the ``glue_local_predictions_1d()`` method from the ``GPSat.postprocessing`` module to glue the two predictions smoothly to yield a single prediction.\n",
    "\n",
    "This is achieved by a gating mechanism, which considers a Gaussian-weighted average of the two predictions.\n",
    "\n",
    "First, we record our results into a pandas dataframe as follows. This dataframe should have as columns (1) the prediction locations, (2) local expert locations, and (3) any results we wish to glue such as the predicted mean and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPSat.postprocessing import glue_local_predictions_1d\n",
    "\n",
    "# Prediction locations for GP1 + GP2\n",
    "pred_locs = list(X_test_1) + list(X_test_2)\n",
    "\n",
    "# Expert locations for GP1 + GP2\n",
    "expert_locs = [xpert_loc_1 for _ in X_test_1] + [xpert_loc_2 for _ in X_test_2]\n",
    "\n",
    "# Predictions from GP1 + GP2\n",
    "f_mean = list(f_mean_1) + list(f_mean_2)\n",
    "f_var = list(f_var_1) + list(f_var_2)\n",
    "\n",
    "# Put these information together into a dataframe\n",
    "results_df = pd.DataFrame({'pred_locs': pred_locs, 'xprt_locs': expert_locs, 'f_mean': f_mean, 'f_var': f_var})\n",
    "\n",
    "print(results_df.head())\n",
    "print(\" \"*20 + \":\")\n",
    "print(\" \"*20 + \":\")\n",
    "print(results_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now glue local predictions by running ``glue_local_predictions_1d()``. This returns a dataframe containing the results of a single *glued prediction*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glue predictions\n",
    "glued_preds = glue_local_predictions_1d(preds_df = results_df,                  # The dataframe where results are stored\n",
    "                                        pred_loc_col = 'pred_locs',             # The column in dataframe corresponding to the prediction locations\n",
    "                                        xprt_loc_col = 'xprt_locs',             # The column in dataframe corresponding to the local expert locations\n",
    "                                        vars_to_glue = ['f_mean', 'f_var'],     # The columns in dataframe corresponding to the predictions\n",
    "                                        inference_radius = inference_radius)    # The inference radius (by passing a single float, it is assumed to be equal for both regions)\n",
    "\n",
    "glued_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we plot the results of this glued prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract mean and variance of glued prediction\n",
    "f_mean = glued_preds[\"f_mean\"]\n",
    "f_var = glued_preds[\"f_var\"]\n",
    "f_std = np.sqrt(f_var)\n",
    "X_test = glued_preds['pred_locs']\n",
    "\n",
    "# Plot results\n",
    "plt.plot(X_grid, f_truth, 'k', zorder=0, label='Ground truth')\n",
    "plt.plot(X_test, f_mean, color='C3', zorder=1, label='Glued predictions')\n",
    "plt.fill_between(X_test, f_mean-1.96*f_std, f_mean+1.96*f_std, color='C3', alpha=0.3)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the glued prediction looks slightly better than our first attempt using a global GP. This improvement is also reflected in the metrics, with a slightly improved log-likelihood score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean squared error: {np.mean((f_truth - f_mean)**2):.4f}\")\n",
    "print(f\"Mean log likelihood: {scipy.stats.norm.logpdf(f_truth, f_mean, f_std).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPSat modelling with local GP experts: A 1-D case study (Part 2)\n",
    "In the previous part of the tutorial, we implemented a local GP expert model to fit on non-stationary data. Here, we will do the same except using ``GPSat``'s ``LocalExpertOI`` class, which automates some of the procedures involved making experiments less cumbersome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import os\n",
    "import GPSat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from GPSat import get_parent_path\n",
    "from GPSat.postprocessing import glue_local_predictions_1d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the same data as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate data\n",
    "N = 100\n",
    "noise_std = 0.05\n",
    "\n",
    "X_grid = np.linspace(0.1, 0.6, 100)\n",
    "X = np.random.uniform(0.1, 0.6, (N,))\n",
    "f = lambda x: np.sin(1/x)\n",
    "epsilon = noise_std * np.random.randn(N)\n",
    "\n",
    "y = f(X) + epsilon\n",
    "f_truth = f(X_grid) # Ground truth\n",
    "\n",
    "# Plot\n",
    "plt.plot(X_grid, f_truth, 'k', zorder=1, label='Ground truth')\n",
    "plt.scatter(X, y, color='C3', alpha=0.6, zorder=2, label='Noisy observations')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration dataclasses\n",
    "\n",
    "We will now conduct the same experiments as in the previous tutorial using ``GPSat.local_experts.LocalExpertOI``.\n",
    "\n",
    "First, we break down a single experiment into the following four key components:\n",
    "\n",
    "1. The local expert locations\n",
    "2. The GP model assigned to each local expert\n",
    "3. The training data\n",
    "4. The points where we want to make predictions\n",
    "\n",
    "In ``GPSat``, we configure each of these four components with a so-called *configuration dataclass*. The goal is to allow sufficient modelling flexibility to accomodate various problems and datasets.\n",
    "\n",
    "### 1. Local expert config\n",
    "We start by setting the configuration for the local expert locations. This can be done by assigning a dataframe containing the locations of the local experts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPSat.config_dataclasses import DataConfig, ModelConfig, PredictionLocsConfig, ExpertLocsConfig\n",
    "\n",
    "# Construct a data frame containing the two local expert locations\n",
    "xpert_loc_1 = 0.25\n",
    "xpert_loc_2 = 0.45\n",
    "xpert_locs_df = pd.DataFrame({'x': [xpert_loc_1, xpert_loc_2]})\n",
    "\n",
    "# Set up an expert location configuration dataclass\n",
    "expert_loc_config = ExpertLocsConfig(source=xpert_locs_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``source`` argument is where we point to the expert locations. In this case, we simply used a dataframe to represent the expert locations and pointed to that. However in more advanced applications, we also have the functionality to instead point to a file where the expert locations are saved, which can be more convenient.\n",
    "\n",
    "### 2. Model config\n",
    "Next, we set up the configuration for the model assigned to each local expert. Here, we will use the ``sklearnGPRModel``, which we specify as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up configuration for the model\n",
    "model_config = ModelConfig(oi_model=\"sklearnGPRModel\",\n",
    "                           init_params={\"likelihood_variance\": noise_std**2,\n",
    "                                        \"kernel\": 'RBF',\n",
    "                                        \"verbose\": False}\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specified the model we are using in ``oi_model`` (pre-implemented ``GPSat`` models can be referred to by strings), and in ``init_params``, we pass any arguments used to initialise the model (expressed as a dictionary). Note that we *do not need* to specify arguments to set the data here (namely ``data``, ``coords`` and ``obs``) as this will be done automatically in the main loop.\n",
    "\n",
    "There are also functionalities to specify constraints on parameters, re-scale the data, etc... however, we will ignore these for the sake of keeping the presentation simple.\n",
    "\n",
    "### 3. Data config\n",
    "Next we set up the configuration for data. Here, we configure information such as the source of data and instructions on how to assign a subset of the data to each local expert.\n",
    "\n",
    "First, we put our training data into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data as dataframe\n",
    "data_df = pd.DataFrame({'x': X, 'y': y})\n",
    "\n",
    "data_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the local data selection, we want to select data points within ± the training radius from the expert locations.\n",
    "\n",
    "In ``GPSat``, we have a unique API to select data from simple instructions. These instructions are expressed in a dictionary with the keys ``\"col\"``, ``\"comp\"`` and ``\"val\"``. For example, see below for the instructions to select data within ± the inference radius of some reference point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set inference radius\n",
    "training_radius = 0.15\n",
    "\n",
    "local_select_instructions = [{\"col\": \"x\", \"comp\": \"<=\", \"val\": training_radius},\n",
    "                             {\"col\": \"x\", \"comp\": \">=\", \"val\": -training_radius}]\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument ``\"col\"`` indicates which column in the dataframe we want to impose conditions on (in this case ``\"x\"``), the ``\"comp\"`` arguments specifies a relation such as \"greater than\", \"less than\", etc... and the ``\"val\"`` argument specifies the value with which we want to compare our column with (in our case, the training radius).\n",
    "\n",
    "Thus programmatically, the above list of commands will select data as follows:\n",
    "\n",
    "```\n",
    ">>> data_1 = data_df[ (data_df[\"x\"] - ref_point) <= training_radius ]\n",
    ">>> data_2 = data_df[ (data_df[\"x\"] - ref_point) >= -training_radius ]\n",
    ">>> local_data = union(data_1, data_2)\n",
    "```\n",
    "\n",
    "Here, ``ref_point`` is some reference point, which, in the main loop, will correspond to the expert locations. The command ``union`` is a pseudo-function to take the intersection of members in ``data_1`` and ``data_2``.\n",
    "\n",
    "With this data selection instruction specified, we can now set the configuration for data as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data config\n",
    "data_config = DataConfig(data_source = data_df,\n",
    "                         obs_col = [\"y\"],\n",
    "                         coords_col = [\"x\"],\n",
    "                         local_select = local_select_instructions\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument ``data_source`` points to the dataframe where our data is stored, ``obs_col`` specifies the column in our dataframe corresponding to the measurements, ``coords_col`` specifies the column corresponding to the input coordinates, and ``local_select`` is where we put our instructions for local data selection.\n",
    "\n",
    "### 4. Prediction location config\n",
    "Finally, we configure the prediction locations. This should include information about the test locations and the local inference region, where the local experts make predictions. The inference region is simply set to be a circular region around the expert location, with radius given by the inference radius.\n",
    "\n",
    "First, we write the prediction locations into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up prediction locations as a dataframe\n",
    "prediction_locs = X_grid\n",
    "prediction_locs_df = pd.DataFrame({'x': X_grid})\n",
    "\n",
    "prediction_locs_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now set the configuration for prediction locations. We take the inference radius to be slightly larger than the training radius to include predictions on the boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_radius = training_radius + 1e-8\n",
    "\n",
    "pred_loc_config = PredictionLocsConfig(method = \"from_dataframe\",\n",
    "                                       df = prediction_locs_df,\n",
    "                                       max_dist = inference_radius\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the ``method`` argument specifies how the prediction locations are selected. In our case this is ``from_dataframe`` and we specify the dataframe in the argument ``df``. The ``max_dist`` argument specifies the inference radius around the expert location.\n",
    "\n",
    "## Run experiment\n",
    "We are now in shape to run our experiment. To do this, we initialise a ``LocalExpertOI`` object from the four config classes we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPSat.local_experts import LocalExpertOI\n",
    "\n",
    "# Set up local expert experiment\n",
    "locexp = LocalExpertOI(data_config = data_config,\n",
    "                       model_config = model_config,\n",
    "                       expert_loc_config = expert_loc_config,\n",
    "                       pred_loc_config = pred_loc_config)\n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just need to specify a path where we want to store our results and run an experiment with the ``run()`` method. The stored path should be a HDF5 file, which uses the extension `\".h5\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to store results\n",
    "store_path = get_parent_path(\"results\", \"1d_tutorial_example.h5\")\n",
    "\n",
    "# for the purposes of a simple example, if store_path exists: delete it\n",
    "if os.path.exists(store_path):\n",
    "    os.remove(store_path)\n",
    "    \n",
    "# run local expert optimal interpolation\n",
    "locexp.run(store_path=store_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the results from the HDF5 with the ``local_experts.get_results_from_h5file()`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract, store in dict\n",
    "dfs, _ = GPSat.local_experts.get_results_from_h5file(store_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the results that are stored by accessing the keys of the dictionary ``dfs``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see tables storing the model parameters (``'kernel_variance'``, ``'lengthscales'``, ``'likelihood_variance'``), the full configuration used to run the experiment stored in json format (``'oi_config'``), model predictions (``'preds'``) and details of the experiment run such as run time, device name, etc... (``'run_details'``).\n",
    "\n",
    "Let's check the ``'preds'`` table storing the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['preds'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous tutorial, we can glue overlapping predictions from different experts by running the ``glue_local_predictions_1d()`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glued_preds = glue_local_predictions_1d(preds_df = dfs['preds'],\n",
    "                                        pred_loc_col = 'pred_loc_x',\n",
    "                                        xprt_loc_col = 'x',\n",
    "                                        vars_to_glue = ['f*', 'f*_var'],\n",
    "                                        inference_radius = inference_radius)\n",
    "glued_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the results below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract glued mean and variance predictions\n",
    "f_mean = glued_preds['f*']\n",
    "f_var = glued_preds['f*_var']\n",
    "f_std = np.sqrt(f_var)\n",
    "X_test = glued_preds['pred_loc_x']\n",
    "\n",
    "# Plot results\n",
    "plt.plot(X_grid, f_truth, 'k', zorder=0, label='Ground truth')\n",
    "plt.plot(X_test, f_mean, color='C3', zorder=1, label='Glued predictions (2 experts)')\n",
    "plt.fill_between(X_test, f_mean-1.96*f_std, f_mean+1.96*f_std, color='C3', alpha=0.3)\n",
    "\n",
    "xvals = [0.25, 0.45]\n",
    "yvals = [-1.2, -1.]\n",
    "plt.errorbar(xvals, yvals, xerr=0.15, fmt='o', elinewidth=2, barsabove=True, capsize=5, alpha=0.5, label='Local expert locations')\n",
    "for (x, y) in zip(xvals, yvals):\n",
    "    plt.vlines(x, -1.4, y, linestyles='dashed')\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([-1.4, 1.1])\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we have also illustrated the local expert locations (blue circle) and the inference regions (blue horizontal bars).\n",
    "\n",
    "Below, we assess the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean squared error: {np.mean((f_truth - f_mean)**2):.4f}\")\n",
    "print(f\"Mean log likelihood: {scipy.stats.norm.logpdf(f_truth, f_mean, f_std).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using more local experts\n",
    "Finally, let's see what happens when we double the number of local experts. Below, we set up the configurations for an experiment using the expert locations at x = [0.2, 0.3, 0.4, 0.5] and training radius = 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set new expert locations\n",
    "xprt_locs = [0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# Set training and inference radii\n",
    "training_radius = 0.1\n",
    "inference_radius = training_radius + 1e-8\n",
    "\n",
    "# Set up configs\n",
    "expert_loc_config = ExpertLocsConfig(source=pd.DataFrame({'x': xprt_locs}))\n",
    "\n",
    "model_config = ModelConfig(oi_model=\"sklearnGPRModel\",\n",
    "                           init_params={\"likelihood_variance\": noise_std**2,\n",
    "                                        \"kernel\": 'RBF',\n",
    "                                        \"verbose\": False}\n",
    "                           )\n",
    "\n",
    "data_config = DataConfig(data_source=data_df,\n",
    "                         obs_col=[\"y\"],\n",
    "                         coords_col=[\"x\"],\n",
    "                         local_select=[{\"col\": \"x\", \"comp\": \"<=\", \"val\": training_radius},\n",
    "                                       {\"col\": \"x\", \"comp\": \">=\", \"val\": -training_radius}]\n",
    "                        )\n",
    "\n",
    "pred_loc_config = PredictionLocsConfig(method=\"from_dataframe\",\n",
    "                                       df=prediction_locs_df,\n",
    "                                       max_dist=inference_radius\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run this experiment below using ``LocalExpertOI``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up local expert experiment\n",
    "locexp = LocalExpertOI(data_config = data_config,\n",
    "                       model_config = model_config,\n",
    "                       expert_loc_config = expert_loc_config,\n",
    "                       pred_loc_config = pred_loc_config)\n",
    "\n",
    "# path to store results\n",
    "store_path = get_parent_path(\"results\", \"1d_tutorial_example.h5\")\n",
    "\n",
    "# for the purposes of a simple example, if store_path exists: delete it\n",
    "if os.path.exists(store_path):\n",
    "    os.remove(store_path)\n",
    "    \n",
    "# run local expert optimal interpolation\n",
    "locexp.run(store_path=store_path, store_every=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract, store in dict\n",
    "dfs, _ = GPSat.local_experts.get_results_from_h5file(store_path)\n",
    "\n",
    "glued_preds = glue_local_predictions_1d(preds_df = dfs['preds'],\n",
    "                                        pred_loc_col = 'pred_loc_x',\n",
    "                                        xprt_loc_col = 'x',\n",
    "                                        vars_to_glue = ['f*', 'f*_var'],\n",
    "                                        inference_radius = inference_radius)\n",
    "\n",
    "# Extract glued mean and variance predictions\n",
    "f_mean = glued_preds['f*']\n",
    "f_var = glued_preds['f*_var']\n",
    "f_std = np.sqrt(f_var)\n",
    "X_test = glued_preds['pred_loc_x']\n",
    "\n",
    "# Plot results\n",
    "plt.plot(X_grid, f_truth, 'k', zorder=0, label='Ground truth')\n",
    "plt.plot(X_test, f_mean, color='C4', zorder=1, label='Glued predictions (4 experts)')\n",
    "plt.fill_between(X_test, f_mean-1.96*f_std, f_mean+1.96*f_std, color='C4', alpha=0.3)\n",
    "\n",
    "xvals = [0.2, 0.3, 0.4, 0.5]\n",
    "yvals = [-1.3, -1.2, -1.1, -1.]\n",
    "plt.errorbar(xvals, yvals, xerr=0.1, fmt='o', elinewidth=2, barsabove=True, capsize=5, alpha=0.5, label='Local expert locations')\n",
    "for (x, y) in zip(xvals, yvals):\n",
    "    plt.vlines(x, -1.4, y, linestyles='dashed')\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([-1.4, 1.1])\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the results look much better and this is also reflected in the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean squared error: {np.mean((f_truth - f_mean)**2):.4f}\")\n",
    "print(f\"Mean log likelihood: {scipy.stats.norm.logpdf(f_truth, f_mean, f_std).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** To achieve the best performance using local experts model, each local experts should have sufficiently many data points to prevent overfitting on a particular region. However, if this happens, we can prevent this by *hyperparameter smoothing*.\n",
    "\n",
    "**Note:** In ``GPSat``, we have not yet considered learning the optimal distribution of expert locations and the corresponding inference/training radii that best fit the data. We typically assume the expert locations to be distributed on an even grid and use the same inference/training at every expert locations. However it might be interesting in the future to consider the learning of such hyperparameters to further improve performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D interpolation of ABC satellite data\n",
    "Now, we look at a more practical example to generate gridded predictions of the sea-ice freeboard from satellite data using ``GPSat``. For the satellite data, we use a sample of the Sentinel 3A and 3B data as well as CryoSat-2 data (referred to as A, B and C respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from GPSat import get_data_path, get_parent_path\n",
    "from GPSat.dataprepper import DataPrep\n",
    "from GPSat.utils import WGS84toEASE2_New, EASE2toWGS84_New, cprint, grid_2d_flatten, get_weighted_values\n",
    "from GPSat.local_experts import LocalExpertOI, get_results_from_h5file\n",
    "from GPSat.plot_utils import plot_pcolormesh, get_projection, plot_pcolormesh_from_results_data\n",
    "from GPSat.config_dataclasses import DataConfig, ModelConfig, PredictionLocsConfig, ExpertLocsConfig\n",
    "from GPSat.postprocessing import glue_local_predictions_2d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in the raw ABC satellite data saved in this repo for demo purposes. We combine the data sources into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all the *_RAW.csv files in data/example\n",
    "# - get files to read\n",
    "raw_files = [get_data_path(\"example\", i)\n",
    "             for i in os.listdir(get_data_path(\"example\")) if re.search(\"_RAW\\.csv$\", i)]\n",
    "\n",
    "# read in, add source col\n",
    "tmp = []\n",
    "for file in raw_files:\n",
    "    source = re.sub(\"_RAW\\.csv$\", \"\", os.path.basename(file))\n",
    "    sat_data = pd.read_csv(file)\n",
    "    sat_data['source'] = source\n",
    "    tmp.append(sat_data)\n",
    "df = pd.concat(tmp)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the variable ``z`` signifies the freeboard measurement that we wish to interpolate. For convenience, we will convert the ``lon``, ``lat`` variables into ``x``, ``y`` EASE(2) grid coordinates and the ``datetime`` column to ``t`` UTC days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert lon, lat, datetime to x, y, t - to be used as the coordinate space\n",
    "# - x,y are in meters, t in days\n",
    "df['x'], df['y'] = WGS84toEASE2_New(lon=df['lon'], lat=df['lat'], lat_0=90, lon_0=0)\n",
    "df['t'] = df['datetime'].values.astype(\"datetime64[D]\").astype(float)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We bin the raw data to a 50km grid to reduce the total data size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin by date, source to a 50x50km grid\n",
    "# - returns a DataSet\n",
    "bin_ds = DataPrep.bin_data_by(df=df.loc[(df['z'] > -0.35) & (df['z'] < 0.65)],\n",
    "                              by_cols=['t', 'source'],\n",
    "                              val_col='z',\n",
    "                              x_col='x',\n",
    "                              y_col='y',\n",
    "                              grid_res=50_000,\n",
    "                              x_range=[-4500000.0, 4500000.0],\n",
    "                              y_range=[-4500000.0, 4500000.0])\n",
    "\n",
    "# convert bin data to DataFrame\n",
    "# - removing all the nans that would be added at grid locations away from data\n",
    "bin_df = bin_ds.to_dataframe().dropna().reset_index()\n",
    "\n",
    "bin_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we plot the binned data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will plot all observations, some on top of each other\n",
    "bin_df['lon'], bin_df['lat'] = EASE2toWGS84_New(bin_df['x'], bin_df['y'])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=get_projection('north'))\n",
    "\n",
    "plot_pcolormesh(ax=ax,\n",
    "                lon=bin_df['lon'],\n",
    "                lat=bin_df['lat'],\n",
    "                plot_data=bin_df['z'],\n",
    "                title=\"example: binned obs\",\n",
    "                scatter=True,\n",
    "                s=20,\n",
    "                fig=fig,\n",
    "                extent=[-180, 180, 60, 90])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up configurations\n",
    "Now, we set up the configuration dataclasses for interpolating this data using local GP experts. For the expert locations, we use points on an evenly spaced 200km grid. For this tutorial, we will only consider interpolating on a small square region around the pole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - spaced every 200km for some x,y range\n",
    "xy_grid = grid_2d_flatten(x_range=[-500000.0, 500000.0],\n",
    "                          y_range=[-500000.0, 500000.0],\n",
    "                          step_size=200_000)\n",
    "\n",
    "# store in dataframe\n",
    "eloc = pd.DataFrame(xy_grid, columns=['x', 'y'])\n",
    "\n",
    "# add a time coordinate\n",
    "eloc['t'] = np.floor(df['t'].mean())\n",
    "\n",
    "print(\"Local expert locations:\")\n",
    "eloc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot expert locations\n",
    "eloc['lon'], eloc['lat'] = EASE2toWGS84_New(eloc['x'], eloc['y'])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=get_projection('north'))\n",
    "\n",
    "plot_pcolormesh(ax=ax,\n",
    "                lon=eloc['lon'],\n",
    "                lat=eloc['lat'],\n",
    "                plot_data=eloc['t'],\n",
    "                title=\"expert locations\",\n",
    "                scatter=True,\n",
    "                s=20,\n",
    "                extent=[-180, 180, 60, 90])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the prediction locations, we use points on a finer 5km grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - spaced every 5km\n",
    "xy_grid = grid_2d_flatten(x_range=[-500000.0, 500000.0],\n",
    "                          y_range=[-500000.0, 500000.0],\n",
    "                          step_size=5_000)\n",
    "\n",
    "# store in dataframe\n",
    "# NOTE: the missing 't' coordinate will be determine by the expert location\n",
    "# - alternatively the prediction location can be specified\n",
    "ploc = pd.DataFrame(xy_grid, columns=['x', 'y'])\n",
    "\n",
    "# Add lon-lat measurements\n",
    "ploc['lon'], ploc['lat'] = EASE2toWGS84_New(ploc['x'], ploc['y'])\n",
    "\n",
    "print(\"Prediction locations:\")\n",
    "ploc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we plot this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot prediction locations\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=get_projection('north'))\n",
    "\n",
    "plot_pcolormesh(ax=ax,\n",
    "                lon=ploc['lon'],\n",
    "                lat=ploc['lat'],\n",
    "                plot_data=np.full(len(ploc), 1.0), #np.arange(len(ploc)),\n",
    "                title=\"prediction locations\",\n",
    "                scatter=True,\n",
    "                s=0.1,\n",
    "                extent=[-180, 180, 60, 90])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set up configurations for Local Expert OI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training and inference radius\n",
    "training_radius = 300_000   # 300km\n",
    "inference_radius = 200_000  # 200km\n",
    "\n",
    "# Local expert locations config\n",
    "local_expert = ExpertLocsConfig(source = eloc)\n",
    "\n",
    "# Model config\n",
    "model = ModelConfig(oi_model = \"GPflowGPRModel\", # Use GPflow GPR model\n",
    "                    init_params = {\n",
    "                        # normalise xy coordinates by 50km\n",
    "                        \"coords_scale\": [50_000, 50_000, 1]\n",
    "                        },\n",
    "                    constraints = {\n",
    "                        # set bounds on the lengthscale hyperparameters\n",
    "                        \"lengthscales\": {\n",
    "                            \"low\": [1e-08, 1e-08, 1e-08],\n",
    "                            \"high\": [600_000, 600_000, 9]\n",
    "                        }\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "# Data config\n",
    "data = DataConfig(data_source = bin_df,\n",
    "                  obs_col = \"z\",\n",
    "                  coords_col = [\"x\", \"y\", \"t\"],\n",
    "                  local_select = [\n",
    "                    # Select data within 300km and ± 4 days of the expert location\n",
    "                    {\"col\": \"t\", \"comp\": \"<=\", \"val\": 4},\n",
    "                    {\"col\": \"t\", \"comp\": \">=\", \"val\": -4},\n",
    "                    {\"col\": [\"x\", \"y\"], \"comp\": \"<\", \"val\": training_radius}\n",
    "                  ]\n",
    "                )\n",
    "\n",
    "# Prediction locs config\n",
    "pred_loc = PredictionLocsConfig(method = \"from_dataframe\",\n",
    "                                df = ploc,\n",
    "                                max_dist = inference_radius)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Local Expert OI object from these configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locexp = LocalExpertOI(expert_loc_config = local_expert,\n",
    "                       data_config = data,\n",
    "                       model_config = model,\n",
    "                       pred_loc_config = pred_loc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify a file to store results and run the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to store results\n",
    "store_path = get_parent_path(\"results\", \"inline_example.h5\")\n",
    "\n",
    "# for the purposes of a simple example, if store_path exists: delete it\n",
    "if os.path.exists(store_path):\n",
    "    cprint(f\"removing: {store_path}\")\n",
    "    os.remove(store_path)\n",
    "\n",
    "# run optimal interpolation\n",
    "locexp.run(store_path=store_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the results from the store path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract, store in dict\n",
    "dfs, _ = get_results_from_h5file(store_path)\n",
    "\n",
    "print(f\"tables in results file: {list(dfs.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_data = dfs[\"preds\"]\n",
    "\n",
    "preds_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before there are going to be overlapping predictions coming from different local experts hence we glue them together using the ``glue_local_predictions_2d()`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple local experts may make predictions at the same prediction location (pred_loc).\n",
    "# - for each prediction at a given location, take we weighted combination\n",
    "# - weights being a function of the distance to each local expert that made a prediction at a given location.\n",
    "\n",
    "plt_data = glue_local_predictions_2d(preds_df=preds_data,\n",
    "                                     pred_loc_cols=[\"pred_loc_x\", \"pred_loc_y\"],\n",
    "                                     xprt_loc_cols=[\"x\", \"y\"],\n",
    "                                     vars_to_glue=[\"f*\", \"f*_var\"],\n",
    "                                     inference_radius=inference_radius)\n",
    "\n",
    "plt_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then plot this result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add convert x,y to lon,lat\n",
    "plt_data['lon'], plt_data['lat'] = EASE2toWGS84_New(plt_data['pred_loc_x'], plt_data['pred_loc_y'])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=get_projection('north'))\n",
    "plot_pcolormesh_from_results_data(ax=ax,\n",
    "                                  dfs={\"preds\": plt_data},\n",
    "                                  table='preds',\n",
    "                                  val_col=\"f*\",\n",
    "                                  scatter=False,\n",
    "                                  x_col='pred_loc_x',\n",
    "                                  y_col='pred_loc_y',\n",
    "                                  fig=fig,\n",
    "                                  plot_kwargs={\"title\": \"f*: predictions\"})\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
