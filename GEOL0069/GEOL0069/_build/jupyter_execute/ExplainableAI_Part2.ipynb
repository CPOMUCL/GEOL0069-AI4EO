{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable AI Part 2\n",
    "Week 9 materials can be accessed [here](https://drive.google.com/drive/folders/1V87Oz-Bc1j8I39RBR8U5UcAZu-VrjYxS?usp=sharing).\n",
    "\n",
    "Today we will go through some more examples of Explainable AI (XAI) and our focus will be on the regression models we've talked about: Polynomial regression, Neural networks and Gaussian Processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Processes\n",
    "\n",
    "Recall Gaussian Processes use several different types of kernels depending on the user's choice, we've metioned RBF kernel, Matern kernel, nerual network kernel and periodic kernel and so on. The Automatic Relevance Determination (ARD) kernel is one specific type of kernel used within Gaussian Processes as well. It is particularly interesting because it can help provide feature importances in some way. \n",
    "\n",
    "##Â ARD Kernel Explained\n",
    "The ARD kernel is a variation of the squared exponential (SE) kernel, which is also known as the radial basis function (RBF) kernel as we have introduced. The standard SE/RBF kernel uses a single length scale parameter to control how quickly the similarity between points decreases with distance. In contrast, the ARD kernel uses a separate length scale parameter for each input dimension (feature). This allows the GP to adjust the influence of each feature independently, leading to a more flexible model that can better capture the underlying structure of the data.\n",
    "\n",
    "## Feature Importance with ARD\n",
    "The concept of feature importances emerges naturally from the ARD kernel. Since each feature has its own length scale parameter, you can interpret the inverse of these length scale values as indicating the importance of the corresponding features: a shorter length scale means that the model is more sensitive to changes in that feature, implying that the feature is more important for predicting the output.\n",
    "\n",
    "In practice, after fitting a Gaussian Process model with an ARD kernel to your data, you can examine the learned length scales to gain insights into which features are most important. This can be particularly useful for feature selection, understanding the data, and improving model interpretability.\n",
    "\n",
    "## User case\n",
    "We have tried to use GPs on predicting sea ice concentration (SIC) so now we test it based on that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features_path = '/content/drive/MyDrive/GEOL0069/Week_6/reshaped_array_condition_21.npy'\n",
    "targets_path = '/content/drive/MyDrive/GEOL0069/Week_6/SICavg_condition.npy'\n",
    "\n",
    "input_features = np.load(features_path)\n",
    "target_variables = np.load(targets_path)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_features, target_variables, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Gpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "import numpy as np\n",
    "# Define a GP model with an ARD kernel\n",
    "kernel = GPy.kern.RBF(input_dim=21, ARD=True)\n",
    "num_inducing = 100\n",
    "model = GPy.models.SparseGPRegression(X_train, y_train.reshape(-1, 1), kernel, num_inducing=num_inducing)\n",
    "\n",
    "# Fit the model (optimize the hyperparameters)\n",
    "model.optimize(messages=True)\n",
    "\n",
    "# Retrieve the learned length scales\n",
    "length_scales = model.kern.lengthscale\n",
    "\n",
    "print(length_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "# Plotting the inverse of the length scales to compare with feature importances directly\n",
    "plt.bar(range(len(length_scales)), 1/np.abs(length_scales), align='center')\n",
    "plt.xticks(range(len(length_scales)), ['Feature %d' % i for i in range(len(length_scales))], rotation=45)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Inverse Length Scale')\n",
    "plt.title('Inverse Feature Importance from Gaussian Process ARD Kernel')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable AI in Polynomial Regression\n",
    "\n",
    "Polynomial regression extends linear regression by incorporating polynomial terms, making the model capable of capturing non-linear relationships between the independent variables and the dependent variable. While this increases the model's flexibility and fit to complex datasets, it introduces challenges in interpreting the model, especially regarding the importance of each feature.\n",
    "\n",
    "## Feature Importance in Polynomial Regression\n",
    "\n",
    "In the context of polynomial regression, understanding feature importance becomes more complex than in linear regression. In a linear model, which can be considered a polynomial regression of degree 1, the coefficients associated with each feature directly reflect the feature's importance. The magnitude and direction of each coefficient indicate how changes in a feature affect the target variable.\n",
    "\n",
    "However, polynomial regression of a degree higher than 1 involves:\n",
    "- **Linear terms**: The original features.\n",
    "- **Polynomial terms**: Each feature raised to higher powers (e.g., squared, cubed).\n",
    "- **Interaction terms**: Combinations of different features.\n",
    "\n",
    "This transformation means we cannot simply look at coefficients to gauge feature importance because each original feature contributes to the model in multiple, complex ways.\n",
    "\n",
    "### Assessing Feature Importance\n",
    "\n",
    "For polynomial models of degree higher than 1, permutation feature importance is a useful tool. It evaluates the model's performance degradation when the values of each feature are shuffled. This degradation indicates the feature's importance. However, it's important to note that this method measures the importance of all terms derived from an original feature (polynomial, interaction terms) rather than the original feature itself.\n",
    "\n",
    "## Limitations and Acknowledgements\n",
    "\n",
    "It's important to acknowledge a key limitation in extracting feature importance from polynomial regression models. Directly interpreting feature importance from the model's coefficients is only straightforward for linear models (degree 1 polynomial regression). For models involving higher-degree polynomials, the interpretation of feature importance must consider the complexity introduced by polynomial and interaction terms. \n",
    "\n",
    "While methods like permutation feature importance can provide insights, they do so for the transformed dataset rather than directly for the original features. This means that while we can understand the importance of the features in the context of the model, translating this back to the original features requires careful consideration and is not as direct as in linear regression.\n",
    "\n",
    "Let's now try to see if we can get feature importances in degree 1 polynomial regression (Linear regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming X_train and y_train are your training data and targets\n",
    "polynomial_features = PolynomialFeatures(degree=1)\n",
    "X_poly_train = polynomial_features.fit_transform(X_train)\n",
    "\n",
    "# Fit a linear regression model\n",
    "model_linear = LinearRegression()\n",
    "model_linear.fit(X_poly_train, y_train)\n",
    "\n",
    "# Get the coefficients as feature importances\n",
    "feature_importances = model_linear.coef_\n",
    "\n",
    "# Visualize the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting the magnitude of the coefficients\n",
    "plt.bar(range(len(feature_importances)), np.abs(feature_importances), align='center')\n",
    "plt.xticks(range(len(feature_importances)), ['Feature %d' % i for i in range(len(feature_importances))], rotation=45)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Coefficient Magnitude')\n",
    "plt.title('Feature Importances from Linear Regression')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "For neural networks, what methods can you think of? Actually the sensitivity analysis we've adopted for CNNs can be used in the same way here. Please note that deep learning models like this are usually not as interepretable compared to probabilistic models like GPs and regression. However, we still can get some insights from it and it may infer the same thing as the other model suggests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def create_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=input_shape),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(1)  \n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def sensitivity_analysis(model, input_data, class_idx):\n",
    "    input_data_tensor = tf.convert_to_tensor(input_data, dtype=tf.float32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_data_tensor)\n",
    "        predictions = model(input_data_tensor, training=False)\n",
    "        class_output = predictions[:, class_idx]\n",
    "\n",
    "    gradients = tape.gradient(class_output, input_data_tensor)\n",
    "    feature_sensitivity = tf.reduce_sum(tf.abs(gradients), axis=0)\n",
    "    return feature_sensitivity.numpy()\n",
    "\n",
    "num_models = 5  \n",
    "ensemble_sensitivities = []\n",
    "\n",
    "for i in range(num_models):\n",
    "    model = create_model((X_train.shape[1],))  \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)  \n",
    "\n",
    "    sample_data = X_test[:1]  \n",
    "    class_idx = 0  \n",
    "\n",
    "    feature_sensitivity = sensitivity_analysis(model, sample_data, class_idx)\n",
    "    ensemble_sensitivities.append(feature_sensitivity)\n",
    "\n",
    "average_sensitivity = np.mean(ensemble_sensitivities, axis=0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(X_train.shape[1]), average_sensitivity)\n",
    "plt.xlabel('Feature Number')\n",
    "plt.ylabel('Average Sensitivity Score')\n",
    "plt.title('Average Sensitivity of Prediction to Each Feature Across Ensemble')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}