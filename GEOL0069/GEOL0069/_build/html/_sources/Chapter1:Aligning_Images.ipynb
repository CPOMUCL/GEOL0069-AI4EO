{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aligning Sentinel-2 and Sentinel-3 OLCI Data\n",
    "\n",
    "You can find the Colab version of this notebook [here](https://drive.google.com/drive/folders/1-aQbcIaohFr4PzzkuCd3k_IXy_Yjw8QP?usp=sharing).In our previous session, we explored the method of retrieving colocated Sentinel-2 optical data alongside Sentinel-3 OLCI data. Building upon that foundation, this week's focus shifts towards the detailed processing and precise alignment of these datasets. Our ultimate goal is to achieve pixel-level colocation between the datasets, enhancing the accuracy and utility of our analysis.\n",
    "\n",
    "## Step 0: Importing Sentinel-2 and Sentinel-3 OLCI Data\n",
    "\n",
    "At this juncture, we reintroduce some previously utilised code with a pivotal objective: to import raw data and transform it into a format amenable to our analysis requirements. Given the extensive memory demands of this operation, we recommend a conceptual understanding of the process rather than direct execution. This step serves as a critical foundation, ensuring that subsequent analyses are built on a robust and standardised dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install rasterio\n",
    "! pip install netCDF4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import os\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Define the path to the main folder where your data is stored.\n",
    "# You need to replace 'path/to/data' with the actual path to your data folder.\n",
    "main_folder_path = '/content/drive/MyDrive/GEOL0069/Assignment_Material'\n",
    "# main_folder_path = './'\n",
    "# This part of the code is responsible for finding all directories in the main_folder that end with '.SEN3'.\n",
    "# '.SEN3' is the format of the folder containing specific satellite data files (in this case, OLCI data files).\n",
    "directories = [d for d in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, d)) and d.endswith('002.SEN3')] #load OLCI imagery\n",
    "\n",
    "# Loop over each directory (i.e., each set of data) found above.\n",
    "for directory in directories:\n",
    "    # Construct the path to the OLCI data file within the directory.\n",
    "    # This path is used to access the data files.\n",
    "    OLCI_file_p = os.path.join(main_folder_path, directory)\n",
    "\n",
    "    # Print the path to the current data file being processed.\n",
    "    # This is helpful for tracking which file is being processed at any time.\n",
    "    print(f\"Processing: {OLCI_file_p}\")\n",
    "\n",
    "    # Load the instrument data from a file named 'instrument_data.nc' inside the directory.\n",
    "    # This file contains various data about the instrument that captured the satellite data.\n",
    "    instrument_data = netCDF4.Dataset(OLCI_file_p + '/instrument_data.nc')\n",
    "    solar_flux = instrument_data.variables['solar_flux'][:]  # Extract the solar flux data.\n",
    "    detector_index = instrument_data.variables['detector_index'][:]  # Extract the detector index.\n",
    "\n",
    "    # Load tie geometries from a file named 'tie_geometries.nc'.\n",
    "    # Tie geometries contain information about viewing angles, which are important for data analysis.\n",
    "    tie_geometries = netCDF4.Dataset(OLCI_file_p + '/tie_geometries.nc')\n",
    "    SZA = tie_geometries.variables['SZA'][:]  # Extract the Solar Zenith Angle (SZA).\n",
    "\n",
    "    # Create a directory for saving the processed data using the original directory name.\n",
    "    # This directory will be used to store output files.\n",
    "    save_directory = os.path.join('path/to/save', directory)\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    # This loop processes each radiance band in the OLCI data.\n",
    "    # OLCI instruments capture multiple bands, each representing different wavelengths.\n",
    "    OLCI_data = []\n",
    "    for Radiance in range(1, 22):  # There are 21 bands in OLCI data.\n",
    "    # for Radiance in [2,5,8,16]:  # selecting relevant bands\n",
    "\n",
    "        Rstr = \"%02d\" % Radiance  # Formatting the band number.\n",
    "        solar_flux_band = solar_flux[Radiance - 1]  # Get the solar flux for the current band.\n",
    "\n",
    "        # Print information about the current band being processed.\n",
    "        # This includes the band number and its corresponding solar flux.\n",
    "        print(f\"Processing Band: {Rstr}\")\n",
    "        print(f\"Solar Flux for Band {Rstr}: {solar_flux_band}\")\n",
    "\n",
    "        # Load radiance values from the OLCI data file for the current band.\n",
    "        OLCI_nc = netCDF4.Dataset(OLCI_file_p + '/Oa' + Rstr + '_radiance.nc')\n",
    "        radiance_values = np.asarray(OLCI_nc['Oa' + Rstr + '_radiance'])\n",
    "\n",
    "        # Initialize an array to store angle data, which will be calculated based on SZA.\n",
    "        angle = np.zeros_like(radiance_values)\n",
    "        for x in range(angle.shape[1]):\n",
    "            angle[:, x] = SZA[:, int(x/64)]\n",
    "\n",
    "        # Calculate the Top of Atmosphere Bidirectional Reflectance Factor (TOA BRF) for the current band.\n",
    "        TOA_BRF = (np.pi * radiance_values) / (solar_flux_band[detector_index] * np.cos(np.radians(angle)))\n",
    "\n",
    "        # Add the calculated TOA BRF data to the OLCI_data list.\n",
    "        OLCI_data.append(TOA_BRF)\n",
    "\n",
    "    reshaped_array = np.moveaxis(np.array(OLCI_data), 0, -1)\n",
    "    OLCI_coord = netCDF4.Dataset(OLCI_file_p + '/geo_coordinates.nc')\n",
    "    OLCI_lon=OLCI_coord['longitude']\n",
    "    OLCI_lat=OLCI_coord['latitude']\n",
    "\n",
    "x_s3, y_s3 = transform(out_proj, in_proj, OLCI_lon, OLCI_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This takes ~60 seconds and ~2Gb RAM\n",
    "\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths to the band images\n",
    "path = '' # You need to specify the path\n",
    "base_path = \"S2A_MSIL1C_20190301T235611_N0207_R116_T01WCU_20190302T014622.SAFE/GRANULE/L1C_T01WCU_A019275_20190301T235610/IMG_DATA/\"\n",
    "bands_paths = {\n",
    "    'B4': path + base_path + 'T01WCU_20190301T235611_B04.jp2',\n",
    "    'B3': path + base_path + 'T01WCU_20190301T235611_B03.jp2',\n",
    "    'B2': path + base_path + 'T01WCU_20190301T235611_B02.jp2'\n",
    "}\n",
    "\n",
    "# Read and stack the band images\n",
    "band_data = []\n",
    "for band in ['B4', 'B3', 'B2']:\n",
    "    with rasterio.open(bands_paths[band]) as src:\n",
    "        band_data.append(src.read(1))\n",
    "\n",
    "# Stack bands and create a mask for valid data (non-zero values in all bands)\n",
    "band_stack = np.dstack(band_data)\n",
    "valid_data_mask = np.all(band_stack > 0, axis=2)\n",
    "\n",
    "# Reshape for GMM, only including valid data\n",
    "X = band_stack[valid_data_mask].reshape((-1, 3))\n",
    "\n",
    "s2_filename = path + './S2A_MSIL1C_20190301T235611_N0207_R116_T01WCU_20190302T014622.SAFE/GRANULE/L1C_T01WCU_A019275_20190301T235610/IMG_DATA/T01WCU_20190301T235611_B04.jp2'\n",
    "\n",
    "# Read the Sentinel-2 image and its geospatial information\n",
    "with rasterio.open(s2_filename) as src:\n",
    "    # Read the raster data and the affine transformation\n",
    "    s2_data = src.read(1)\n",
    "    transform_matrix = src.transform\n",
    "\n",
    "    # Get the spatial reference system (CRS)\n",
    "    srs = src.crs\n",
    "\n",
    "# Create grid of X,Y values\n",
    "rows, cols = s2_data.shape\n",
    "x_s2, y_s2 = [], []\n",
    "for row in range(rows):\n",
    "    print(row)\n",
    "    for col in range(cols):\n",
    "        x, y = transform_matrix * (col, row)\n",
    "        x_s2.append(x)\n",
    "        y_s2.append(y)\n",
    "\n",
    "# Convert grid of X,Y values to latitude/longitude\n",
    "in_proj = Proj(init=str(srs))  # Initialize projection from CRS\n",
    "out_proj = Proj(proj='latlong')  # Initialize projection for latitude/longitude\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading processed data\n",
    "As mentioned, you don't need to run the above cells. But you will need to load them from your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "path = '/content/drive/MyDrive/Teaching_Michel/GEOL0069/StudentFolder/Week_4/' # You need to specify the path\n",
    "x_s2=np.load(path+'x_s2.npy')\n",
    "y_s2=np.load(path+'y_s2.npy')\n",
    "band_stack=np.load(path+'band_stack.npy')\n",
    "\n",
    "x_s3=np.load(path+'x_s3.npy')\n",
    "y_s3=np.load(path+'y_s3.npy')\n",
    "reshaped_array=np.load(path+'reshaped_array.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also verify if Sentinel-2 data is indeed within Sentinel-3 OLCI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ''\n",
    "s1=100 #subsampling rate for S2\n",
    "s2=100 #subsampling rate for s3\n",
    "\n",
    "plt.scatter(x_s3[::s1,::s1],y_s3[::s1,::s1])\n",
    "plt.scatter(np.asarray(x_s2).reshape(10980,10980)[::s2,::s2],np.asarray(y_s2).reshape(10980,10980)[::s2,::s2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = (x_s3 > 360000) & (x_s3 < 380000) & (y_s3 > 7800000) & (y_s3 < 7820000)\n",
    "plt.scatter(x_s3[condition],y_s3[condition],c=reshaped_array[condition,0],vmin=0.7,vmax=1)\n",
    "plt.colorbar()\n",
    "plt.savefig(path+'OLCI_zoom.png',dpi=1200)\n",
    "\n",
    "#This takes 9 minutes (perhaps saving the figure takes the longest)\n",
    "x_s2bis=np.asarray(x_s2).reshape(10980,10980)\n",
    "y_s2bis=np.asarray(y_s2).reshape(10980,10980)\n",
    "condition = (x_s2bis > 360000) & (x_s2bis < 380000) & (y_s2bis > 7800000) & (y_s2bis < 7820000)\n",
    "plt.scatter(x_s2bis[condition],y_s2bis[condition],c=band_stack[condition,2]/10000,vmin=0.7,vmax=1)\n",
    "plt.colorbar()\n",
    "plt.savefig(path+'S2_zoom.png',dpi=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s2_condition = x_s2bis[condition]\n",
    "y_s2_condition = y_s2bis[condition]\n",
    "band_stack_condition = band_stack[condition,2]/10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Overlap optimisation\n",
    "Now we optimize the overlap between Sentinel-2 image and Senitnel-3 image and better align them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_array_rolled = np.roll(reshaped_array, 3, axis=0)\n",
    "# Roll x_s3_rolled array by -5 grids in the y-direction\n",
    "reshaped_array_rolled = np.roll(reshaped_array_rolled, -5, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code that interpolates on a common grid before doing matcing of images in terms of colours and correlating to find offset\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import correlate2d\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "from skimage import exposure  # Import exposure module from skimage\n",
    "\n",
    "# Scatter input values\n",
    "condition = (x_s3 > 360000) & (x_s3 < 380000) & (y_s3 > 7800000) & (y_s3 < 7820000)\n",
    "x_s3_condition, y_s3_condition = x_s3[condition], y_s3[condition]\n",
    "# reshaped_array_condition = reshaped_array[condition, 0]\n",
    "reshaped_array_condition = reshaped_array_rolled[condition, 0]\n",
    "# x_s2_condition, y_s2_condition, band_stack_condition = x_s2[condition], y_s2[condition], band_stack[condition]\n",
    "\n",
    "# Define grid coordinates\n",
    "ngrid=400 #here it means grid is 20000/400=50m\n",
    "x_grid = np.linspace(min(x_s3_condition.min(), x_s2_condition.min()), max(x_s3_condition.max(), x_s2_condition.max()), ngrid)\n",
    "y_grid = np.linspace(min(y_s3_condition.min(), y_s2_condition.min()), max(y_s3_condition.max(), y_s2_condition.max()), ngrid)\n",
    "x_grid, y_grid = np.meshgrid(x_grid, y_grid)\n",
    "\n",
    "# Interpolate scattered values onto grid\n",
    "ss=10 #only use every 10th point of S2 images\n",
    "z1_grid = griddata((x_s2_condition[::ss], y_s2_condition[::ss]), band_stack_condition[::ss], (x_grid, y_grid), method='cubic')\n",
    "z2_grid = griddata((x_s3_condition, y_s3_condition), reshaped_array_condition, (x_grid, y_grid), method='cubic')\n",
    "\n",
    "\n",
    "# Perform histogram matching\n",
    "matched_z2_grid = exposure.match_histograms(z2_grid, z1_grid)\n",
    "\n",
    "# Find the translation that optimizes overlap using cross-correlation\n",
    "z1_grid[z1_grid>0.75]=1\n",
    "z1_grid[z1_grid<0.75]=0.7\n",
    "z2_grid[z2_grid>0.75]=1\n",
    "z2_grid[z2_grid<0.75]=0.7\n",
    "matched_z2_grid[matched_z2_grid>0.75]=1\n",
    "matched_z2_grid[matched_z2_grid<0.75]=0.7\n",
    "\n",
    "#Without the binary classification above I have found that the correlation doesn't work to find alignment\n",
    "z1_grid_no_nan = np.nan_to_num(z1_grid, nan=1)\n",
    "matched_z2_grid_no_nan = np.nan_to_num(matched_z2_grid, nan=1)\n",
    "corr = correlate2d(z1_grid_no_nan, matched_z2_grid_no_nan, mode='same', boundary='wrap')  # Compute cross-correlation\n",
    "y, x = np.unravel_index(np.argmax(corr), corr.shape)  # Find the indices of the maximum correlation\n",
    "dx = x - matched_z2_grid.shape[1] // 2  # Compute the x-offset\n",
    "dy = y - matched_z2_grid.shape[0] // 2  # Compute the y-offset\n",
    "matched_z2_grid_aligned = np.roll(np.roll(matched_z2_grid_no_nan, dy, axis=0), dx, axis=1)\n",
    "\n",
    "\n",
    "print(\"Optimal translation (dx, dy):\", dx, dy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_z2_grid_aligned.shape\n",
    "plt.pcolor(z1_grid_no_nan,vmin=0.7,vmax=1)\n",
    "plt.savefig(path+'S2_binary_grid_50m.png',dpi=400)\n",
    "plt.show()\n",
    "\n",
    "plt.pcolor(matched_z2_grid_aligned,vmin=0.7,vmax=1)\n",
    "plt.savefig(path+'S3_binary_matched_S2_grid_50m.png',dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code does colocation within grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "\n",
    "condition = (x_s3 > 360000) & (x_s3 < 380000) & (y_s3 > 7800000) & (y_s3 < 7820000)\n",
    "x_s3_condition, y_s3_condition = x_s3[condition], y_s3[condition]\n",
    "\n",
    "# Define a KD-tree using x_s2_condition and y_s2_condition\n",
    "tree = cKDTree(list(zip(x_s2_condition, y_s2_condition)))\n",
    "\n",
    "# Query the tree to find all points within x_s3_condition and y_s3_condition grids (Choose one of the following 2 methods)\n",
    "# Option 1:\n",
    "indices_within_grid = tree.query_ball_point(list(zip(x_s3_condition, y_s3_condition)), r=300.0)\n",
    "# Option 2: Query for a square neighbourhood\n",
    "max_radius=1000\n",
    "indices_within_grid = tree.query(list(zip(x_s3_condition[::100], y_s3_condition[::100])), k=len(x_s2_condition), p=40, distance_upper_bound=max_radius)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot one colocated S2 data for one S3 cell (Within a radius of 300m)\n",
    "plt.scatter(x_s2_condition[indices_within_grid[10][0:500]],y_s2_condition[indices_within_grid[10][0:500]],c=band_stack_condition[indices_within_grid[0][0:500]]/10000)#,vmin=0.7,vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Finding collocated pixels\n",
    "### Option 1: Using KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter input values\n",
    "# condition = (x_s3 > 360000) & (x_s3 < 380000) & (y_s3 > 7800000) & (y_s3 < 7820000)\n",
    "# x_s3_condition, y_s3_condition = x_s3[condition], y_s3[condition]\n",
    "\n",
    "# Define a KD-tree using x_s2_condition and y_s2_condition\n",
    "tree = cKDTree(list(zip(x_s2_condition, y_s2_condition)))\n",
    "\n",
    "# Query the tree to find all points within x_s3_condition and y_s3_condition grids\n",
    "ss3=1\n",
    "indices_within_grid = tree.query_ball_point(list(zip(x_s3_condition[::ss3], y_s3_condition[::ss3])), r=300.0)\n",
    "\n",
    "#There are 2826 S2 pixels within 300 m radius of pixel 500 of S3\n",
    "len(indices_within_grid[25])\n",
    "\n",
    "#Here are the indices of the S2 pixels within 300 m of pixel 500 of S3\n",
    "x_s2_condition[indices_within_grid[25]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And here they are plotted\n",
    "index_s3=1200\n",
    "plt.scatter(x_s2_condition[indices_within_grid[index_s3]],y_s2_condition[indices_within_grid[index_s3]],c=band_stack_condition[indices_within_grid[index_s3]])#,vmin=0.,vmax=1)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(band_stack_condition[indices_within_grid[index_s3]].ravel(),bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And here they are plotted\n",
    "plt.scatter(x_s2_condition[indices_within_grid[25]],y_s2_condition[indices_within_grid[25]],c=band_stack_condition[indices_within_grid[25]]/10000)#,vmin=0.7,vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using KDTree with square box search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition = (x_s3 > 360000) & (x_s3 < 380000) & (y_s3 > 7800000) & (y_s3 < 7820000)\n",
    "# x_s3_condition, y_s3_condition = x_s3[condition], y_s3[condition]\n",
    "\n",
    "# Define a KD-tree using x_s2_condition and y_s2_condition\n",
    "tree = cKDTree(list(zip(x_s2_condition, y_s2_condition)))\n",
    "\n",
    "# Query the tree to find all points within x_s3_condition and y_s3_condition grids\n",
    "max_radius=300\n",
    "# indices_within_grid = tree.query(list(zip(x_s3_condition[::100], y_s3_condition[::100])), k=len(x_s2_condition), p=40, distance_upper_bound=max_radius)[1]\n",
    "indices_within_grid = tree.query(list(zip(x_s3_condition[::100], y_s3_condition[::100])), k=10000, p=40, distance_upper_bound=max_radius)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And here they are plotted\n",
    "xtest=x_s2_condition[indices_within_grid[25][indices_within_grid[25]<3996001]]\n",
    "ytest=y_s2_condition[indices_within_grid[25][indices_within_grid[25]<3996001]]\n",
    "ztest=band_stack_condition[indices_within_grid[25][indices_within_grid[25]<3996001]]/10000\n",
    "plt.scatter(xtest,ytest,c=ztest)#,vmin=0.7,vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Using rtree and polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install rtree\n",
    "! pip install rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon, Point\n",
    "from rtree import index\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import matplotlib.patches as mpatches\n",
    "from pyproj import Proj, transform\n",
    "\n",
    "\n",
    "# Create an R-tree spatial index\n",
    "idx = index.Index()\n",
    "\n",
    "x_s3=x_s3[100:900:s1,2600:3310]\n",
    "y_s3=y_s3[100:900:s1,2600:3310]\n",
    "# Insert bounding boxes of pixels into the R-tree index\n",
    "for i in range(x_s3.shape[0] - 1):\n",
    "    print(i)\n",
    "    for j in range(x_s3.shape[1] - 1):\n",
    "        pixel_x_min = x_s3[i, j]\n",
    "        pixel_x_max = x_s3[i+1, j+1]\n",
    "        pixel_y_min = y_s3[i, j]\n",
    "        pixel_y_max = y_s3[i+1, j+1]\n",
    "\n",
    "        # Create a polygon for the current pixel\n",
    "        pixel_polygon = Polygon([(pixel_x_min, pixel_y_min), (pixel_x_max, pixel_y_min),\n",
    "                                 (pixel_x_max, pixel_y_max), (pixel_x_min, pixel_y_max)])\n",
    "\n",
    "        # Insert the polygon into the R-tree index\n",
    "        idx.insert(i * x_s3.shape[1] + j, pixel_polygon.bounds)\n",
    "\n",
    "# Initialize a list to store indices of points within each pixel\n",
    "indices_within_pixel = []\n",
    "\n",
    "# Loop through each point in x_s2 and y_s2 and check if it's within any pixel polygon\n",
    "ss2=100\n",
    "for i, (x, y) in enumerate(zip(x_s2[::ss2], y_s2[::ss2])):\n",
    "    # print(i)\n",
    "    point = Point(x, y)\n",
    "    # Find the indices of polygons (pixels) that intersect with the point\n",
    "    intersected_pixels = list(idx.intersection((x, y, x, y)))\n",
    "\n",
    "    # Check if the point is within any of the intersected pixels\n",
    "    for pixel_idx in intersected_pixels:\n",
    "        pixel_polygon = Polygon([(x_s3[pixel_idx // x_s3.shape[1], pixel_idx % x_s3.shape[1]], y_s3[pixel_idx // x_s3.shape[1], pixel_idx % x_s3.shape[1]]),\n",
    "                                 (x_s3[pixel_idx // x_s3.shape[1], pixel_idx % x_s3.shape[1] + 1], y_s3[pixel_idx // x_s3.shape[1], pixel_idx % x_s3.shape[1] + 1]),\n",
    "                                 (x_s3[pixel_idx // x_s3.shape[1] + 1, pixel_idx % x_s3.shape[1] + 1], y_s3[pixel_idx // x_s3.shape[1] + 1, pixel_idx % x_s3.shape[1] + 1]),\n",
    "                                 (x_s3[pixel_idx // x_s3.shape[1] + 1, pixel_idx % x_s3.shape[1]], y_s3[pixel_idx // x_s3.shape[1] + 1, pixel_idx % x_s3.shape[1]])])\n",
    "        if point.within(pixel_polygon):\n",
    "            indices_within_pixel.append(pixel_idx)\n",
    "\n",
    "# Convert the list of indices to a numpy array\n",
    "indices_within_pixel = np.array(indices_within_pixel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to extract projection and coordinates (lon,lat,x,y) of S2 imagery\n",
    "#This takes ~60 seconds and ~2Gb RAM\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from pyproj import Proj, transform\n",
    "\n",
    "# Path to the Sentinel-2 JP2 image file\n",
    "path = '/content/drive/MyDrive/Teaching_Michel/GEOL0069/StudentFolder/Week_4/' # You need to specify the path\n",
    "s2_filename = path + './S2A_MSIL1C_20190301T235611_N0207_R116_T01WCU_20190302T014622.SAFE/GRANULE/L1C_T01WCU_A019275_20190301T235610/IMG_DATA/T01WCU_20190301T235611_B04.jp2'\n",
    "\n",
    "# Read the Sentinel-2 image and its geospatial information\n",
    "with rasterio.open(s2_filename) as src:\n",
    "    # Read the raster data and the affine transformation\n",
    "    s2_data = src.read(1)\n",
    "    transform_matrix = src.transform\n",
    "\n",
    "    # Get the spatial reference system (CRS)\n",
    "    srs = src.crs\n",
    "\n",
    "# Convert grid of X,Y values to latitude/longitude\n",
    "in_proj = Proj(init=str(srs))  # Initialize projection from CRS\n",
    "out_proj = Proj(proj='latlong')  # Initialize projection for latitude/longitude\n",
    "x_s3, y_s3 = transform(out_proj, in_proj, OLCI_lon, OLCI_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create grid of X,Y values\n",
    "rows, cols = s2_data.shape\n",
    "x_s2, y_s2 = [], []\n",
    "for row in range(rows):\n",
    "    print(row)\n",
    "    for col in range(cols):\n",
    "        x, y = transform_matrix * (col, row)\n",
    "        x_s2.append(x)\n",
    "        y_s2.append(y)\n",
    "\n",
    "# Convert grid of X,Y values to latitude/longitude\n",
    "in_proj = Proj(init=str(srs))  # Initialize projection from CRS\n",
    "out_proj = Proj(proj='latlong')  # Initialize projection for latitude/longitude\n",
    "# lon_s2, lat_s2 = transform(in_proj, out_proj, x_s2, y_s2)\n",
    "# x_s2, y_s2 = transform(out_proj, in_proj, lon_s2, lat_s2)\n",
    "\n",
    "sar_dataset = rasterio.open(sar_path)\n",
    "dem_dataset = rasterio.open(matched_filePath[0])\n",
    "\n",
    "###############################################################################\n",
    "# Function calculate_metrics_within_sar_pixel\n",
    "###############################################################################\n",
    "def calculate_metrics_within_sar_pixel(sar_dataset, dem_dataset,lowerPercentileLeft,upperPercentileLeft,lowerPercentileRight,upperPercentileRight):\n",
    "\n",
    "    # Determine the overlap area\n",
    "    sar_bounds = sar_dataset.bounds\n",
    "    dem_bounds = dem_dataset.bounds\n",
    "    overlap_bounds = (max(sar_bounds.left, dem_bounds.left), max(sar_bounds.bottom, dem_bounds.bottom),\n",
    "                      min(sar_bounds.right, dem_bounds.right), min(sar_bounds.top, dem_bounds.top))\n",
    "\n",
    "    sar_window = sar_dataset.window(*overlap_bounds)\n",
    "    sar_window = sar_window.round_lengths(op='ceil')\n",
    "\n",
    "    # List to store SAR pixel values and roughness values\n",
    "    sar_metrics_list = []\n",
    "\n",
    "    for i in range(int(sar_window.row_off), int(sar_window.row_off + sar_window.height)):\n",
    "        for j in range(int(sar_window.col_off), int(sar_window.col_off + sar_window.width)):\n",
    "            # Read SAR pixel value\n",
    "            sar_pixel_value = sar_dataset.read(1, window=rasterio.windows.Window(j, i, 1, 1))\n",
    "            sar_pixel_value_clean = np.nan if (sar_pixel_value == SAR_MISSING_DATA_VALUE).all() else sar_pixel_value\n",
    "\n",
    "            # Calculate the geographic boundary of the SAR pixel\n",
    "            sar_pixel_bounds = sar_dataset.window_bounds(((i, i+1), (j, j+1)))\n",
    "            dem_window = dem_dataset.window(*sar_pixel_bounds)\n",
    "            dem_data = dem_dataset.read(1, window=dem_window)\n",
    "\n",
    "            # Replace DEM missing data values with np.nan\n",
    "            dem_data[dem_data == DEM_MISSING_DATA_VALUE] = np.nan\n",
    "\n",
    "            # Compute metrics if valid DEM data is present\n",
    "            if not np.isnan(dem_data).all():\n",
    "                rms = np.sqrt(np.nanmean(np.square(dem_data)))\n",
    "\n",
    "                pLower_left = np.nanpercentile(dem_data, lowerPercentileLeft)\n",
    "                pUpper_left = np.nanpercentile(dem_data, upperPercentileLeft)\n",
    "                percentile_difference_left = pUpper_left - pLower_left\n",
    "\n",
    "                pLower_right = np.nanpercentile(dem_data, lowerPercentileRight)\n",
    "                pUpper_right = np.nanpercentile(dem_data, upperPercentileRight)\n",
    "                percentile_difference_right = pUpper_right - pLower_right\n",
    "\n",
    "                nPoints = np.count_nonzero(~np.isnan(dem_data))\n",
    "\n",
    "                sar_pixel_value_clean = np.nan if (sar_pixel_value == SAR_MISSING_DATA_VALUE).all() else sar_pixel_value.item()\n",
    "                sar_metrics_list.append((sar_pixel_value_clean, rms, percentile_difference_left, percentile_difference_right, nPoints))\n",
    "\n",
    "    return sar_metrics_list\n",
    "    \n",
    "# Calculate RMS values and get SAR pixel values\n",
    "sar_metrics_list = calculate_metrics_within_sar_pixel(\n",
    "    sar_dataset, dem_dataset,\n",
    "    lowerPercentileLeft, upperPercentileLeft,\n",
    "    lowerPercentileRight, upperPercentileRight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This takes ~60 seconds and 5Gb RAM\n",
    "\n",
    "\n",
    "import os\n",
    "# import netCDF4\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Define the path to the main folder where your data is stored.\n",
    "# You need to replace 'path/to/data' with the actual path to your data folder.\n",
    "main_folder_path = '/content/drive/MyDrive/Teaching_Michel/GEOL0069/StudentFolder/Week_4'\n",
    "# main_folder_path = './'\n",
    "# This part of the code is responsible for finding all directories in the main_folder that end with '.SEN3'.\n",
    "# '.SEN3' is the format of the folder containing specific satellite data files (in this case, OLCI data files).\n",
    "directories = [d for d in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, d)) and d.endswith('002.SEN3')] #load OLCI imagery\n",
    "\n",
    "# Loop over each directory (i.e., each set of data) found above.\n",
    "for directory in directories:\n",
    "    # Construct the path to the OLCI data file within the directory.\n",
    "    # This path is used to access the data files.\n",
    "    OLCI_file_p = os.path.join(main_folder_path, directory)\n",
    "\n",
    "    # Print the path to the current data file being processed.\n",
    "    # This is helpful for tracking which file is being processed at any time.\n",
    "    print(f\"Processing: {OLCI_file_p}\")\n",
    "\n",
    "    # This loop processes each radiance band in the OLCI data.\n",
    "    # OLCI instruments capture multiple bands, each representing different wavelengths.\n",
    "    OLCI_data = []\n",
    "    # for Radiance in range(1, 22):  # There are 21 bands in OLCI data.\n",
    "    for Radiance in [2,5,8,16]:  # selecting relevant bands\n",
    "\n",
    "        Rstr = \"%02d\" % Radiance  # Formatting the band number.\n",
    "\n",
    "        # Print information about the current band being processed.\n",
    "        # This includes the band number and its corresponding solar flux.\n",
    "        print(f\"Processing Band: {Rstr}\")\n",
    "\n",
    "        # Load radiance values from the OLCI data file for the current band.\n",
    "        # OLCI_nc = netCDF4.Dataset(OLCI_file_p + '/Oa' + Rstr + '_radiance.nc')\n",
    "        s3_filename = OLCI_file_p + '/Oa' + Rstr + '_radiance.nc'\n",
    "\n",
    "        # Read the Sentinel-2 image and its geospatial information\n",
    "        with rasterio.open(s3_filename) as src_s3:\n",
    "            # Read the raster data and the affine transformation\n",
    "            s3_data = src_s3.read(1)\n",
    "            transform_matrix_s3 = src_s3.transform\n",
    "\n",
    "            # Get the spatial reference system (CRS)\n",
    "            srs_s3 = src_s3.crs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
