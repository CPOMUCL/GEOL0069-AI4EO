{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMi0zTh9y6WL"
   },
   "source": [
    "# AI/Machine Learning Implementation\n",
    "ğŸ“˜ **Interactive Version**: For a hands-on experience with this chapter's content, access the interactive notebook in [Google Colab](https://drive.google.com/file/d/15PZPu8HNLsc3JylevPAUhljWhc309rfh/view?usp=sharing).\n",
    "\n",
    "In this section, we delve into the practical application of prominent AI and machine learning algorithms using the datasets curated in the preceding chapters. While an exhaustive theoretical comprehension isn't mandatory, a foundational grasp of the underlying principles and confidence in their basic implementation will be advantageous.\n",
    "The main task focus on this chapter will be classification because we are doing surface discrimination . We will come across regression in future chapters.\n",
    "\n",
    "## Loading the data\n",
    "From the previous notebook, you should have your training and testing data ready from this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "phULR4RQ7kBy",
    "outputId": "1966fa95-54af-4685-aa84-1f88303a6529"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PaKUFwRU7RUr"
   },
   "outputs": [],
   "source": [
    "save_path = '/content/drive/MyDrive/GEOL0069/2425/Week 2/Week2_AI_Algorithms/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODswjidVXmO-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "X_train = np.load(os.path.join(save_path, 'X_train_balanced.npy'))\n",
    "X_test = np.load(os.path.join(save_path, 'X_test_balanced.npy'))\n",
    "y_train = np.load(os.path.join(save_path, 'y_train_balanced.npy'))\n",
    "y_test = np.load(os.path.join(save_path, 'y_test_balanced.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9DxCvwwIUCp"
   },
   "source": [
    "## Convolutional Neural Networks (CNN)\n",
    "\n",
    "### Introduction to CNNs\n",
    "\n",
    "Convolutional Neural Networks, commonly known as CNNs, are a class of deep neural networks specially designed to process data with grid-like topology, such as images {cite}`Goodfellow-et-al-2016,lecun2015deep`. Originating from the visual cortex's biological processes, CNNs are revolutionising the way we understand and interpret visual data.\n",
    "\n",
    "### Why CNN for Image Data?\n",
    "\n",
    "Traditional neural networks, when used for images, suffer from two main issues:\n",
    "\n",
    "- **Too many parameters**: For a simple 256x256 colored image, an input layer would have \\(256 * 256 * 3 = 196,608\\) neurons, leading to an enormous number of parameters even in the first hidden layer.\n",
    "- **Loss of spatial information**: Flattening an image into a vector for traditional neural networks can lose the spatial hierarchies and patterns in the image, which are often crucial for understanding and interpreting visual data.\n",
    "\n",
    "CNNs address both issues by introducing convolutions.\n",
    "\n",
    "### Key Components of CNN\n",
    "\n",
    "1. **Convolutional Layer** {cite}`lecun2015deep`: This is the core building block of a CNN. It slides a filter (smaller in size than the input data) over the input data (like an image) to produce a feature map or convolved feature. The primary purpose of a convolution is to extract features from the input data.\n",
    "2. **Pooling Layer**: Pooling layers are used to reduce the dimensions of the feature maps, thereby reducing the number of parameters and computation in the network. The most common type of pooling is max pooling.\n",
    "3. **Fully Connected Layer**: After several convolutional and pooling layers, the final classification is done using one or more fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular neural networks.\n",
    "4. **Activation Functions**: Non-linearity is introduced into the CNN using activation functions. The Rectified Linear Unit (ReLU) is the most commonly used activation function in CNNs.\n",
    "\n",
    "### How CNNs Learn Spatial Hierarchies\n",
    "\n",
    "CNNs learn spatial hierarchies automatically. The initial layers might learn to detect edges, the next layers learn to detect shapes by combining edges, further layers might detect more complex structures. This ability to learn spatial hierarchies from raw data gives CNNs their power. It allows them to detect complex objects in images by combining simpler features from the earlier layers.\n",
    "\n",
    "### Advantages of CNNs\n",
    "\n",
    "- **Parameter Sharing**: A feature detector (filter) that's useful in one part of the image can be useful in another part of the image {cite}`krizhevsky2012imagenet`.\n",
    "- **Sparsity of Connections**: In each layer, each output value depends only on a small number of input values, making the computation more efficient.\n",
    "\n",
    "### Basic Code Implementation\n",
    "\n",
    "Below, you'll find a basic Convolutional Neural Network (CNN) structure implemented in TensorFlow. Treat this as a foundational blueprint for your subsequent implementations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E6ovIjRxmvMy",
    "outputId": "857c0d35-57fe-473a-9cdd-820f5a921285"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m501/501\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5559 - loss: 2.9276 - val_accuracy: 0.5903 - val_loss: 0.7663\n",
      "Epoch 2/10\n",
      "\u001b[1m501/501\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6224 - loss: 0.6680 - val_accuracy: 0.6229 - val_loss: 0.6545\n",
      "Epoch 3/10\n",
      "\u001b[1m501/501\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6325 - loss: 0.6538 - val_accuracy: 0.4989 - val_loss: 0.6995\n",
      "Epoch 4/10\n",
      "\u001b[1m501/501\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6702 - loss: 0.5943 - val_accuracy: 0.6476 - val_loss: 0.5719\n",
      "Epoch 5/10\n",
      "\u001b[1m501/501\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6856 - loss: 0.5695 - val_accuracy: 0.7604 - val_loss: 0.4956\n",
      "Epoch 6/10\n",
      "\u001b[1m501/501\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7096 - loss: 0.5473 - val_accuracy: 0.7935 - val_loss: 0.4949\n",
      "Epoch 7/10\n",
      "\u001b[1m501/501\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7167 - loss: 0.5331 - val_accuracy: 0.6235 - val_loss: 0.5956\n",
      "Epoch 8/10\n",
      "\u001b[1m501/501\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6986 - loss: 0.5562 - val_accuracy: 0.7637 - val_loss: 0.4657\n",
      "Epoch 9/10\n",
      "\u001b[1m501/501\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7329 - loss: 0.5321 - val_accuracy: 0.7722 - val_loss: 0.5041\n",
      "Epoch 10/10\n",
      "\u001b[1m501/501\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7026 - loss: 0.5517 - val_accuracy: 0.7632 - val_loss: 0.4525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x783d001d5890>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(3, 3, 21), padding='SAME'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "# Add additional convolutional and pooling layers as needed\n",
    "# ...\n",
    "\n",
    "# Add dense layers for classification\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))  # 10 is the number of classes\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10,\n",
    "        validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5UdQIRSz9n3"
   },
   "source": [
    "## Random Forests\n",
    "\n",
    "Random Forest is a notable and significant part of machine learning and is commonly used for classification. It can also be used for regression, but its application in classification is more prevalent. Decision Trees are the core components of a Random Forest, so let's delve into the concepts of Decision Trees {cite}`breiman2001random,quinlan1986induction`.\n",
    "\n",
    "### Theoretical Foundations\n",
    "\n",
    "### 1. **Ensemble Learning**\n",
    "\n",
    "Ensemble methods employ multiple learning algorithms to achieve better predictive performance than any individual learning algorithm alone {cite}`dietterich2000ensemble`. The primary principle behind ensemble models is that several weak learners come together to form a strong learner.\n",
    "\n",
    "### 2. **Decision Trees**\n",
    "\n",
    "Decision trees are central to a Random Forest. They split data into subsets based on feature values, recursively producing a decision tree {cite}`quinlan1986induction`.\n",
    "\n",
    "### 3. **Bootstrap Aggregating (Bagging)**\n",
    "\n",
    "Random Forests leverage bagging, where multiple dataset subsets are created by drawing samples with replacement. A separate decision tree is built for each of these samples {cite}`breiman1996bagging`.\n",
    "\n",
    "### 4. **Feature Randomness**\n",
    "\n",
    "In conventional decision trees, the best feature is chosen to split data at every node. However, Random Forests introduce randomness by selecting a random set of features, then choosing the best split from this subset, ensuring a diverse ensemble of trees.\n",
    "\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- **Generalisation**: By combining the predictions of multiple trees, Random Forests tend to generalize better and are less susceptible to overfitting on training data.\n",
    "  \n",
    "- **Parallel Processing**: Each decision tree can be built independently, allowing for parallel processing which speeds up the algorithm considerably for large datasets.\n",
    "\n",
    "- **Handling Missing Values**: Random Forests can handle missing values and still produce reasonable predictions.\n",
    "\n",
    "- **Importance Scoring**: They provide an importance score for each feature, aiding in feature selection or interpretability.\n",
    "\n",
    "### Implementation in Python (Using Scikit-learn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHPPt3PDysZS"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialise the model with n_estimators specifying the number of trees in the forest\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# We need to reshape the data in order to be compatible with Random Forest\n",
    "X_reshaped = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "# Fit the model to your training data\n",
    "clf.fit(X_reshaped, y_train)\n",
    "\n",
    "# Predict the classes of the test data\n",
    "X_test_reshaped = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "y_pred = clf.predict(X_test_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19rlMqePQFLu"
   },
   "source": [
    "## Vision Transformer (ViT)\n",
    "\n",
    "Vision Transformers (ViTs) are a recent breakthrough in the field of deep learning for image processing. They depart from the traditional convolutional neural network (CNN) approach and apply transformers, which were originally designed for natural language processing tasks, to image classification.\n",
    "\n",
    "### Theoretical Foundations\n",
    "\n",
    "### 1. **Tokenisation of Images**\n",
    "\n",
    "Instead of processing images using convolutions, ViTs divide the image into fixed-size patches, linearly embed them, and then process the resulting sequence of vectors (or tokens) using a transformer.{cite}`dosovitskiy2020image`\n",
    "\n",
    "### 2. **Position Embeddings**\n",
    "\n",
    "Since the original transformer doesn't have a notion of the relative positions of tokens, positional embeddings are added to the patch embeddings to retain the positional information.{cite}`dosovitskiy2020image`\n",
    "\n",
    "### 3. **Transformer Architecture**\n",
    "\n",
    "The core of ViT is the transformer architecture, which consists of multiple layers of multi-head self-attention mechanisms and feed-forward neural networks.{cite}`dosovitskiy2020image`\n",
    "\n",
    "### 4. **Classification Head**\n",
    "\n",
    "After processing through the transformer layers, the embedding of the first token (often referred to as the 'CLS' token) is used to classify the image.{cite}`dosovitskiy2020image`\n",
    "\n",
    "### Advantages of ViT\n",
    "\n",
    "- **Model Transferability**: ViTs pre-trained on large datasets can be fine-tuned on smaller datasets, achieving high performance even when the available labeled data is limited.\n",
    "\n",
    "- **Scalability**: ViTs are more data-hungry compared to CNNs. However, their performance continues to improve as the model size and the amount of data increase, often surpassing other architectures.\n",
    "\n",
    "- **Flexibility**: The transformer architecture isn't specialized for grid-like data (like images), making ViTs potentially more flexible for varied input data types.\n",
    "\n",
    "### Challenges\n",
    "\n",
    "- **Computational Demand**: ViTs can be computationally intensive, especially when dealing with large images or when the model has many layers.\n",
    "\n",
    "- **Data Requirement**: To achieve optimal performance, ViTs often require more training data compared to CNNs.\n",
    "\n",
    "### Implementation\n",
    "The implmentation of Vision Transformer is much more complicated than CNN and Random Forest as there is no built-in functions or layers in the library. However, the following code uses some existing functions like Muliti-head attention to build the transformer block. You don't need to know the exactly and detailed structure of ViT as it is not required in this course. Please follow the code below for example of implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrEyZ2TNXmPB"
   },
   "outputs": [],
   "source": [
    "# Install packages needed\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "from keras.saving import register_keras_serializable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTgYVrmOXmPB"
   },
   "outputs": [],
   "source": [
    "#=========================================================================================================\n",
    "#=========================================================================================================\n",
    "#=========================================================================================================\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "   for units in hidden_units:\n",
    "       x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "       x = layers.Dropout(dropout_rate)(x)\n",
    "   return x\n",
    "\n",
    "@register_keras_serializable()\n",
    "class Patches(layers.Layer):\n",
    "   def __init__(self, patch_size, **kwargs):\n",
    "       super().__init__(**kwargs)\n",
    "       self.patch_size = patch_size\n",
    "\n",
    "   def call(self, images):\n",
    "       batch_size = tf.shape(images)[0]\n",
    "       patches = tf.image.extract_patches(\n",
    "           images=images,\n",
    "           sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "           strides=[1, self.patch_size, self.patch_size, 1],\n",
    "           rates=[1, 1, 1, 1],\n",
    "           padding=\"VALID\",\n",
    "       )\n",
    "       patch_dims = patches.shape[-1]\n",
    "       patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "       return patches\n",
    "\n",
    "   def get_config(self):\n",
    "       config = super().get_config()\n",
    "       config.update({\"patch_size\": self.patch_size})\n",
    "       return config\n",
    "\n",
    "@register_keras_serializable()\n",
    "class PatchEncoder(layers.Layer):\n",
    "   def __init__(self, num_patches, projection_dim, **kwargs):\n",
    "       super().__init__(**kwargs)\n",
    "       self.num_patches = num_patches\n",
    "       self.projection_dim = projection_dim\n",
    "       self.projection = layers.Dense(units=projection_dim)\n",
    "       self.position_embedding = layers.Embedding(\n",
    "           input_dim=num_patches, output_dim=projection_dim\n",
    "       )\n",
    "\n",
    "   def call(self, patch):\n",
    "       positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "       encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "       return encoded\n",
    "\n",
    "   def get_config(self):\n",
    "       config = super().get_config()\n",
    "       config.update({\n",
    "           \"num_patches\": self.num_patches,\n",
    "           \"projection_dim\": self.projection_dim\n",
    "       })\n",
    "       return config\n",
    "\n",
    "\n",
    "#=========================================================================================================\n",
    "#=========================================================================================================\n",
    "#=========================================================================================================\n",
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = more_data(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model\n",
    "#=========================================================================================================\n",
    "#=========================================================================================================\n",
    "#=========================================================================================================\n",
    "def run_experiment(model):\n",
    "    optimizer = keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Update the filepath to include `.weights.h5`\n",
    "    checkpoint_filepath = \"/tmp/checkpoint.weights.h5\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "        batch_size=30,\n",
    "        epochs=1,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    # Load the best weights\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpcgEEdmXmPC"
   },
   "outputs": [],
   "source": [
    "num_classes = 2 #Can be changed to multi-classed classification\n",
    "input_shape = (3, 3, 21)#depends on the size of the image we want\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 100\n",
    "image_size = 72\n",
    "patch_size = 6\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zs2TetZsXmPC"
   },
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "more_data = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"more_data\",\n",
    ")\n",
    "more_data.layers[0].adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T3KcDUqsXmPC",
    "outputId": "4d6aac0c-6e29-4bc2-d5d2-df7039db4843"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m535/535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 84ms/step - accuracy: 0.8420 - loss: 1.6579 - top-5-accuracy: 1.0000 - val_accuracy: 0.9366 - val_loss: 0.1545 - val_top-5-accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "\u001b[1m535/535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 84ms/step - accuracy: 0.9345 - loss: 0.1784 - top-5-accuracy: 1.0000 - val_accuracy: 0.9456 - val_loss: 0.1205 - val_top-5-accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "\u001b[1m535/535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 80ms/step - accuracy: 0.9411 - loss: 0.1591 - top-5-accuracy: 1.0000 - val_accuracy: 0.9343 - val_loss: 0.1515 - val_top-5-accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "\u001b[1m535/535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 81ms/step - accuracy: 0.9450 - loss: 0.1499 - top-5-accuracy: 1.0000 - val_accuracy: 0.9405 - val_loss: 0.1422 - val_top-5-accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "\u001b[1m535/535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 83ms/step - accuracy: 0.9491 - loss: 0.1448 - top-5-accuracy: 1.0000 - val_accuracy: 0.9579 - val_loss: 0.1049 - val_top-5-accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "\u001b[1m535/535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 83ms/step - accuracy: 0.9453 - loss: 0.1469 - top-5-accuracy: 1.0000 - val_accuracy: 0.9680 - val_loss: 0.0823 - val_top-5-accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "\u001b[1m535/535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 81ms/step - accuracy: 0.9475 - loss: 0.1428 - top-5-accuracy: 1.0000 - val_accuracy: 0.9641 - val_loss: 0.0890 - val_top-5-accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "\u001b[1m535/535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 81ms/step - accuracy: 0.9519 - loss: 0.1407 - top-5-accuracy: 1.0000 - val_accuracy: 0.9646 - val_loss: 0.0939 - val_top-5-accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "\u001b[1m535/535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 82ms/step - accuracy: 0.9519 - loss: 0.1328 - top-5-accuracy: 1.0000 - val_accuracy: 0.9338 - val_loss: 0.1731 - val_top-5-accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "\u001b[1m535/535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 82ms/step - accuracy: 0.9541 - loss: 0.1218 - top-5-accuracy: 1.0000 - val_accuracy: 0.9613 - val_loss: 0.1090 - val_top-5-accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "\u001b[1m535/535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 81ms/step - accuracy: 0.9577 - loss: 0.1130 - top-5-accuracy: 1.0000 - val_accuracy: 0.9602 - val_loss: 0.1067 - val_top-5-accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "\u001b[1m535/535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 82ms/step - accuracy: 0.9554 - loss: 0.1236 - top-5-accuracy: 1.0000 - val_accuracy: 0.9209 - val_loss: 0.3253 - val_top-5-accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "\u001b[1m535/535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 82ms/step - accuracy: 0.9480 - loss: 0.1413 - top-5-accuracy: 1.0000 - val_accuracy: 0.9663 - val_loss: 0.0904 - val_top-5-accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "\u001b[1m535/535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 81ms/step - accuracy: 0.9589 - loss: 0.1157 - top-5-accuracy: 1.0000 - val_accuracy: 0.9675 - val_loss: 0.0918 - val_top-5-accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "\u001b[1m535/535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 81ms/step - accuracy: 0.9649 - loss: 0.1002 - top-5-accuracy: 1.0000 - val_accuracy: 0.9680 - val_loss: 0.0852 - val_top-5-accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "\u001b[1m535/535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 82ms/step - accuracy: 0.9617 - loss: 0.1060 - top-5-accuracy: 1.0000 - val_accuracy: 0.9624 - val_loss: 0.0965 - val_top-5-accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "\u001b[1m535/535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 81ms/step - accuracy: 0.9634 - loss: 0.1083 - top-5-accuracy: 1.0000 - val_accuracy: 0.9540 - val_loss: 0.1070 - val_top-5-accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "\u001b[1m535/535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 81ms/step - accuracy: 0.9644 - loss: 0.1004 - top-5-accuracy: 1.0000 - val_accuracy: 0.9523 - val_loss: 0.1692 - val_top-5-accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "\u001b[1m535/535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 84ms/step - accuracy: 0.9641 - loss: 0.1066 - top-5-accuracy: 1.0000 - val_accuracy: 0.9697 - val_loss: 0.0965 - val_top-5-accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "\u001b[1m535/535\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 84ms/step - accuracy: 0.9686 - loss: 0.0877 - top-5-accuracy: 1.0000 - val_accuracy: 0.9742 - val_loss: 0.0732 - val_top-5-accuracy: 1.0000\n",
      "\u001b[1m62/62\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9700 - loss: 0.0719 - top-5-accuracy: 1.0000\n",
      "Test accuracy: 96.82%\n",
      "Test top 5 accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Here vit is your trained model after the training\n",
    "vit = create_vit_classifier()\n",
    "history = run_experiment(vit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGj-L4lQXmPD"
   },
   "source": [
    "After training the model, you can save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dkRsevKXmPD"
   },
   "outputs": [],
   "source": [
    "vit.save('/content/drive/MyDrive/GEOL0069/2324/Week_2/model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSmPF_dwXmPD"
   },
   "source": [
    "You can always load the model via this line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_I50VePlXmPD"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "loaded_vit = load_model('/content/drive/MyDrive/GEOL0069/2324/Week_2/model.keras', \n",
    "                      custom_objects={'Patches': Patches, \n",
    "                                    'PatchEncoder': PatchEncoder,\n",
    "                                    'more_data': more_data,\n",
    "                                    'mlp': mlp})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65fth0MnXmPD"
   },
   "source": [
    "## Model Selection and Cross-validation\n",
    "Cross-validation and model selection are pivotal components in ensuring the robustness of machine learning models. They ensure that our model doesn't just memorise the training data (overfitting) and that it generalises well to new, unseen data. However, they are some small nuances between this procedure in Deep Learning models (like CNN and ViT) and traditional Machine Learning models (Random Forests)\n",
    "\n",
    "### Deep Learning Models:\n",
    "\n",
    "- **Validation During Training**:\n",
    "    - Deep learning models, particularly when trained on large datasets, often incorporate a validation set during the training process. This is done to monitor the model's generalization performance and to use mechanisms like early stopping, learning rate annealing based on validation loss, etc.\n",
    "\n",
    "- **Cross-Validation Less Common**:\n",
    "    - Due to the computational intensity and time required to train deep learning models, k-fold cross-validation is less commonly used. Instead, a single hold-out validation set (or sometimes a few different validation sets) is used.\n",
    "\n",
    "- **Model Selection**:\n",
    "    - While the principles of model selection apply to deep learning, the specific approach might be different. Hyperparameter tuning in deep learning might involve methods like random search, Bayesian optimization, or dedicated libraries like Optuna or Ray Tune, instead of just grid search.\n",
    "\n",
    "### Traditional ML Models:\n",
    "\n",
    "- **Cross-Validation**:\n",
    "    - For many traditional machine learning models, k-fold cross-validation is a standard technique because these models are typically faster to train. Cross-validation gives a more robust estimate of the model's performance.\n",
    "\n",
    "- **Model Selection**:\n",
    "    - Grid search combined with cross-validation (e.g., `GridSearchCV` in scikit-learn) is a common method for hyperparameter tuning and model selection for traditional algorithms.\n",
    "\n",
    "### Overlap & Best Practices:\n",
    "\n",
    "- Despite these general trends, it's worth noting that there's overlap. Deep learning models can also be evaluated using cross-validation if computational resources permit. Similarly, traditional ML models can (and do) use validation sets during training, especially for iterative algorithms like gradient boosting machines.\n",
    "\n",
    "- The choice between using a validation set during training or relying on cross-validation often depends on the dataset's size, computational resources, and specific project requirements.\n",
    "\n",
    "- Actually we have included validation procedure in the building of CNN and ViT. Check for code that contains 'validation or validation split'.\n",
    "\n",
    "\n",
    "### What is Cross-Validation? {cite}`bishop2006pattern`\n",
    "\n",
    "Cross-validation is a technique to evaluate the performance of a model by splitting the dataset into a training set and a validation set multiple times. The most common method is k-fold cross-validation.\n",
    "### K-Fold Cross-Validation {cite}`bishop2006pattern`\n",
    "\n",
    "In k-fold cross-validation, the training data is randomly partitioned into k equal-sized subsets. Of the k subsets, a single subset is retained as validation data, and the remaining k-1 subsets are used as training data. The cross-validation process is repeated k times, with each of the k subsets used exactly once as validation data. The k results can then be averaged to produce a single estimation.\n",
    "\n",
    "The following code is a example of K-Fold CV for Random Forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqWXD8j4XmPD"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# Assuming `model` is your sklearn model and `X` and `y` are your data\n",
    "# Perform 5-fold cross-validation\n",
    "scores = cross_val_score(clf, X_reshaped, y_train_balanced, cv=5)\n",
    "\n",
    "# Print the mean of the cross-validation scores\n",
    "print(\"Mean cross-validation score: \", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S62UVepHXmPE"
   },
   "source": [
    "## Model Selection\n",
    "\n",
    "Model selection involves choosing the best model from a set of models based on performance. The model with the best cross-validation score is typically selected.\n",
    "\n",
    "### Grid Search\n",
    "\n",
    "A popular technique for model selection is grid search. Grid search involves:\n",
    "\n",
    "- Specifying a subset of the hyperparameter space.\n",
    "- Training a model for each hyperparameter combination.\n",
    "- Evaluating each model using cross-validation.\n",
    "- Selecting the model with the best cross-validation performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ml1xBcTgXmPE"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Example for a hypothetical model's hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20, 30]\n",
    "}\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: \", grid_search.best_score_)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
