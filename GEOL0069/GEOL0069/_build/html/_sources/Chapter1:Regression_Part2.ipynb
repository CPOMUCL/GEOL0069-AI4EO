{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of Regression Techniques in Satellite Imagery Analysis\n",
    "\n",
    "In Week 5, we delved into an array of regression techniques, including polynomial regression, neural networks, and Gaussian processes, each offering unique perspectives and methodologies for modeling complex relationships within data. This week, we pivot our focus towards the practical application of these regression techniques to a challenging yet highly relevant problem in the field of satellite imagery analysis: predicting sea-ice concentration and the fraction of leads/melt ponds. Our dataset comprises 21 spectral bands from satellite imagery, each spanning over 5000 data points, which we aim to regress onto scalar values that may represent sea-ice concentration and lead/melt pond fractions across the same 5000 observations depending on what we want. In the previous notebook, we prepared such a dataset for us to apply the regression techniques. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Let's recall some key phases of our machine learning project cycle:\n",
    "\n",
    "- **Data Collection**: Data is the cornerstone of any ML project. This stage involves gathering necessary data relevant to our problem. The quality, quantity, and variety of data can significantly influence the model's performance. For example, collecting satellite images like those from OLCI represents a common data collection process, with much of the raw data being publicly available online for download.\n",
    "\n",
    "- **Data Preprocessing**: Raw data often requires cleaning and formatting before use. This step includes converting raw data into a format interpretable by ML models, handling missing values, normalizing data, and feature engineering. Previously, we introduced a method for creating a machine learning dataset using IRIS.\n",
    "\n",
    "In the previous notebook, we completed the data collection phase of our cycle. Now, we move to data preprocessing. The primary task here is to split the data into training, validation, and testing sets, which will allow us to evaluate our model's performance after training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_path = '/content/drive/MyDrive/GEOL0069/Week_6/reshaped_array_condition_21.npy'\n",
    "targets_path = '/content/drive/MyDrive/GEOL0069/Week_6/SICavg_condition.npy'\n",
    "\n",
    "input_features = np.load(features_path)\n",
    "target_variables = np.load(targets_path)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_features, target_variables, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression {cite}`draper1998applied`\n",
    "\n",
    "### Recall Polynomial Regression\n",
    "\n",
    "Polynomial regression is a form of regression analysis in which the relationship between the independent variable $x$ and the dependent variable $y$ is modeled as an $n$ th degree polynomial. Polynomial regression fits a nonlinear relationship between the value of $x$ and the corresponding conditional mean of $y$, denoted $E(y |x)$. Below code shows how we can apply it on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "polynomial_features = PolynomialFeatures(degree=2)\n",
    "X_poly_train = polynomial_features.fit_transform(X_train)\n",
    "\n",
    "model_poly = LinearRegression()\n",
    "model_poly.fit(X_poly_train, y_train)\n",
    "\n",
    "X_poly_test = polynomial_features.transform(X_test)\n",
    "\n",
    "y_pred_poly = model_poly.predict(X_poly_test)\n",
    "mse = mean_squared_error(y_test, y_pred_poly)\n",
    "print(f\"The Mean Squared Error (MSE) on the test set is: {mse}\")\n",
    "\n",
    "sample_idx = np.random.choice(np.arange(len(y_test)), size=50, replace=False)\n",
    "plt.scatter(X_test[sample_idx, 0], y_test[sample_idx], color='black', label='Actual')\n",
    "plt.scatter(X_test[sample_idx, 0], y_pred_poly[sample_idx], color='blue', label='Predicted', alpha=0.5)\n",
    "plt.title('Polynomial Regression with Degree 2 - Test Set Prediction')\n",
    "plt.xlabel('First Feature')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks {cite}`goodfellow2016deep`\n",
    "\n",
    "### Recall Important Components of Neural Networks\n",
    "\n",
    "1. **Layers**: Composed of neurons, layers are the fundamental units of neural networks. A fully connected network consists of input, hidden, and output layers.\n",
    "2. **Neurons**: Each neuron in a layer is connected to all neurons in the previous and next layers, processing the input data and passing on its output.\n",
    "3. **Weights and Biases**: These parameters are adjusted during training to minimize the network's error in predicting the target variable.\n",
    "4. **Activation Functions**: Functions like ReLU or Sigmoid introduce non-linearities, allowing the network to model complex relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model_nn = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(21,)),  \n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_nn.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model_nn.fit(X_train, y_train, epochs=10)\n",
    "\n",
    "y_pred = model_nn.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"The Mean Squared Error (MSE) on the test set is: {mse}\")\n",
    "\n",
    "model_nn.summary()\n",
    "\n",
    "y_pred_nn = y_pred.flatten()\n",
    "\n",
    "sample_idx = np.random.choice(np.arange(len(y_test)), size=50, replace=False)\n",
    "\n",
    "plt.scatter(X_test[sample_idx, 0], y_test[sample_idx], color='black', label='Actual')\n",
    "plt.scatter(X_test[sample_idx, 0], y_pred_nn[sample_idx], color='blue', label='Predicted', alpha=0.5)\n",
    "plt.title('Neural Network Regression - Test Set Prediction')\n",
    "plt.xlabel('First Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Processes  {cite}`bishop2006pattern`\n",
    "\n",
    "\n",
    "### Recall Gaussian Processes\n",
    "\n",
    "A Gaussian Process (GP) is essentially an advanced form of a Gaussian (or normal) distribution, but instead of being over simple variables, it's over functions. Imagine a GP as a method to predict or estimate a function based on known data points. Note that we are using sparse GP here as the data we have here is somethat high-dimensional (21 bands). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install GPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "kernel = GPy.kern.RBF(input_dim=21)\n",
    "num_inducing = 100  \n",
    "\n",
    "gp = GPy.models.SparseGPRegression(X_train, y_train.reshape(-1, 1), kernel, num_inducing=num_inducing)\n",
    "\n",
    "gp.optimize(messages=True)\n",
    "\n",
    "y_pred_gp, variance = gp.predict(X_test)\n",
    "y_pred_gp = y_pred.flatten() \n",
    "sigma = np.sqrt(variance).flatten()\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred_gp)\n",
    "print(f\"The Mean Squared Error (MSE) on the test set is: {mse}\")\n",
    "\n",
    "sample_idx = np.random.choice(np.arange(len(y_test)), size=50, replace=False)\n",
    "plt.scatter(X_test[sample_idx, 0], y_test[sample_idx], color='black', label='Actual')\n",
    "plt.scatter(X_test[sample_idx, 0], y_pred_gp[sample_idx], color='blue', label='Predicted', alpha=0.5)\n",
    "plt.title('Gaussian Process Regression - Test Set Prediction')\n",
    "plt.xlabel('First Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Performances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x_min, x_max = X_test[:, 0].min(), X_test[:, 0].max()\n",
    "y_min, y_max = y_test.min(), y_test.max()\n",
    "\n",
    "predictions = [y_pred_poly, y_pred_nn, y_pred_gp]\n",
    "titles = ['Polynomial Regression', 'Neural Network', 'Gaussian Process']\n",
    "\n",
    "for i, y_pred in enumerate(predictions):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sample_idx = np.random.choice(np.arange(len(y_test)), size=50, replace=False)\n",
    "    plt.scatter(X_test[sample_idx, 0], y_test[sample_idx], color='black', label='Actual')\n",
    "    plt.scatter(X_test[sample_idx, 0], y_pred[sample_idx], color='blue', label='Predicted', alpha=0.5)\n",
    "    plt.plot([x_min, x_max], [y_min, y_max], 'r--', label='Perfect Prediction')\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.title(titles[i] + ' - Test Set Prediction')\n",
    "    plt.xlabel('First Feature')\n",
    "    plt.ylabel('Target')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollout\n",
    "Now we can test our model on another part of OLCI image. We will use polynomial regression as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "path1 = '/content/drive/MyDrive/GEOL0069/Week_6/reshaped_array_condition21_rollout.npy'\n",
    "path2 = '/content/drive/MyDrive/GEOL0069/Week_6/x_s3_condition_rollout.npy'\n",
    "path3 = '/content/drive/MyDrive/GEOL0069/Week_6/y_s3_rollout.npy'\n",
    "\n",
    "reshaped_array_condition21_rollout = np.load(path1)\n",
    "x_s3_condition_rollout = np.load(path2)\n",
    "y_s3_condition_rollout = np.load(path3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly_test = polynomial_features.transform(reshaped_array_condition21_rollout)\n",
    "y_pred_poly = model_poly.predict(X_poly_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_s3_condition_rollout,y_s3_condition_rollout,c=y_pred_poly,vmin=0.7,vmax=1)\n",
    "plt.colorbar()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
